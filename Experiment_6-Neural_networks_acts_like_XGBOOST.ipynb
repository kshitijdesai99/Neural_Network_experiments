{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a237a898-1d65-4811-b3b5-4a989d74817a",
   "metadata": {},
   "source": [
    "## I have a tabular dataset, I found XGBOOST gets very high R2 score, Im training neural network to behave like it so that we get high R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a579eb-f1aa-4b48-89dd-404170cd26ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skorch\n",
      "  Downloading skorch-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/calld-admin/Documents/MCP/notebooks/env/lib/python3.12/site-packages (from skorch) (2.1.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /Users/calld-admin/Documents/MCP/notebooks/env/lib/python3.12/site-packages (from skorch) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/calld-admin/Documents/MCP/notebooks/env/lib/python3.12/site-packages (from skorch) (1.15.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/calld-admin/Documents/MCP/notebooks/env/lib/python3.12/site-packages (from skorch) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in /Users/calld-admin/Documents/MCP/notebooks/env/lib/python3.12/site-packages (from skorch) (4.67.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/calld-admin/Documents/MCP/notebooks/env/lib/python3.12/site-packages (from scikit-learn>=0.22.0->skorch) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/calld-admin/Documents/MCP/notebooks/env/lib/python3.12/site-packages (from scikit-learn>=0.22.0->skorch) (3.6.0)\n",
      "Downloading skorch-1.1.0-py3-none-any.whl (228 kB)\n",
      "Installing collected packages: skorch\n",
      "Successfully installed skorch-1.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3676aa12-7cce-48bf-88b6-0b08865f212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.8339\u001b[0m        \u001b[32m0.3810\u001b[0m  4.0089\n",
      "      2        \u001b[36m0.4608\u001b[0m        \u001b[32m0.3693\u001b[0m  3.3376\n",
      "      3        \u001b[36m0.4332\u001b[0m        0.6266  3.2480\n",
      "      4        \u001b[36m0.4124\u001b[0m        0.3706  3.1778\n",
      "      5        \u001b[36m0.4079\u001b[0m        0.3810  3.1768\n",
      "      6        \u001b[36m0.3936\u001b[0m        0.3823  3.1335\n",
      "      7        \u001b[36m0.3777\u001b[0m        0.3888  3.1820\n",
      "      8        0.3797        \u001b[32m0.3044\u001b[0m  3.1744\n",
      "      9        0.3780        0.3234  3.2151\n",
      "     10        \u001b[36m0.3770\u001b[0m        0.3322  3.1595\n",
      "     11        \u001b[36m0.3716\u001b[0m        0.3169  3.4444\n",
      "     12        \u001b[36m0.3694\u001b[0m        0.3164  3.4831\n",
      "     13        \u001b[36m0.3679\u001b[0m        \u001b[32m0.2850\u001b[0m  3.4621\n",
      "     14        0.3856        0.2975  3.1952\n",
      "     15        \u001b[36m0.3663\u001b[0m        0.3942  3.2034\n",
      "     16        \u001b[36m0.3633\u001b[0m        0.3589  3.1812\n",
      "     17        \u001b[36m0.3600\u001b[0m        0.3355  3.2748\n",
      "     18        0.3677        0.2975  3.3868\n",
      "     19        0.3601        0.3010  3.4632\n",
      "     20        0.3614        0.3808  3.4213\n",
      "R² Score: 0.7137992978096008\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "layer1:\n",
      "  Tanh: 33.70%\n",
      "  LeakyReLU: 37.66%\n",
      "  Linear: 28.64%\n",
      "layer2:\n",
      "  Tanh: 37.39%\n",
      "  LeakyReLU: 37.06%\n",
      "  Linear: 25.56%\n",
      "layer3:\n",
      "  Tanh: 47.47%\n",
      "  LeakyReLU: 28.37%\n",
      "  Linear: 24.16%\n",
      "\n",
      "Overall Average:\n",
      "  Tanh: 36.72%\n",
      "  LeakyReLU: 36.16%\n",
      "  Linear: 27.12%\n"
     ]
    }
   ],
   "source": [
    "# !pip install skorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from skorch.dataset import ValidSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class ParallelActivationLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Linear transformations for each activation function\n",
    "        self.linear_tanh = nn.Linear(in_features, out_features)\n",
    "        self.linear_leaky_relu = nn.Linear(in_features, out_features)\n",
    "        self.linear_linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "        # Weights for combining activations (learnable)\n",
    "        self.activation_weights = nn.Parameter(torch.ones(out_features, 3) / 3)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply each linear transformation and activation\n",
    "        out_tanh = self.tanh(self.linear_tanh(x))\n",
    "        out_leaky_relu = self.leaky_relu(self.linear_leaky_relu(x))\n",
    "        out_linear = self.linear_linear(x)\n",
    "        \n",
    "        # Stack the outputs\n",
    "        activations = torch.stack([out_tanh, out_leaky_relu, out_linear], dim=-1)\n",
    "        \n",
    "        # Apply softmax to weights to ensure they sum to 1\n",
    "        weights = torch.softmax(self.activation_weights, dim=-1)\n",
    "        \n",
    "        # Weighted combination\n",
    "        output = torch.sum(activations * weights.unsqueeze(0), dim=-1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_units=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use parallel activation layers\n",
    "        self.layer1 = ParallelActivationLayer(8, num_units)\n",
    "        self.bn1 = nn.BatchNorm1d(num_units)\n",
    "        self.layer2 = ParallelActivationLayer(num_units, num_units//2)\n",
    "        self.bn2 = nn.BatchNorm1d(num_units//2)\n",
    "        self.layer3 = ParallelActivationLayer(num_units//2, num_units//4)\n",
    "        self.bn3 = nn.BatchNorm1d(num_units//4)\n",
    "        self.output = nn.Linear(num_units//4, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.dropout(self.bn1(self.layer1(X)))\n",
    "        X = self.dropout(self.bn2(self.layer2(X)))\n",
    "        X = self.dropout(self.bn3(self.layer3(X)))\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X = X.astype('float32')\n",
    "y = y.astype('float32').reshape(-1, 1)\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    MyModule,\n",
    "    max_epochs=20,\n",
    "    lr=0.01,\n",
    "    optimizer=Adam,\n",
    "    optimizer__weight_decay=1e-4,\n",
    "    batch_size=64,\n",
    "    iterator_train__shuffle=True,\n",
    "    train_split=ValidSplit(cv=0.2),\n",
    "    callbacks=[EarlyStopping(patience=20)],\n",
    "    device='mps',\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('net', net),\n",
    "])\n",
    "\n",
    "pipe.fit(X, y)\n",
    "y_pred = pipe.predict(X)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(f\"R² Score: {r2}\")\n",
    "\n",
    "# Analyze activation function usage\n",
    "def analyze_activation_weights(model):\n",
    "    print(\"\\nActivation Function Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    total_tanh = 0\n",
    "    total_leaky_relu = 0\n",
    "    total_linear = 0\n",
    "    total_neurons = 0\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, ParallelActivationLayer):\n",
    "            weights = torch.softmax(module.activation_weights, dim=-1)\n",
    "            layer_tanh = weights[:, 0].mean().item()\n",
    "            layer_leaky_relu = weights[:, 1].mean().item()\n",
    "            layer_linear = weights[:, 2].mean().item()\n",
    "            \n",
    "            print(f\"{name}:\")\n",
    "            print(f\"  Tanh: {layer_tanh:.2%}\")\n",
    "            print(f\"  LeakyReLU: {layer_leaky_relu:.2%}\")\n",
    "            print(f\"  Linear: {layer_linear:.2%}\")\n",
    "            \n",
    "            num_neurons = weights.shape[0]\n",
    "            total_tanh += layer_tanh * num_neurons\n",
    "            total_leaky_relu += layer_leaky_relu * num_neurons\n",
    "            total_linear += layer_linear * num_neurons\n",
    "            total_neurons += num_neurons\n",
    "    \n",
    "    if total_neurons > 0:\n",
    "        print(f\"\\nOverall Average:\")\n",
    "        print(f\"  Tanh: {total_tanh/total_neurons:.2%}\")\n",
    "        print(f\"  LeakyReLU: {total_leaky_relu/total_neurons:.2%}\")\n",
    "        print(f\"  Linear: {total_linear/total_neurons:.2%}\")\n",
    "\n",
    "# Analyze the trained model\n",
    "analyze_activation_weights(net.module_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37708f52-6a93-474b-a5cc-3b233534ec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Basic XGBoost...\n",
      "Basic XGBoost R² Score: 0.849273\n",
      "\n",
      "Hyperparameter tuning...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Tuned XGBoost R² Score: 0.852921\n",
      "Best parameters: {'subsample': 0.7, 'reg_lambda': 2.0, 'reg_alpha': 0.1, 'n_estimators': 2000, 'min_child_weight': 1, 'max_depth': 7, 'learning_rate': 0.01, 'gamma': 0.2, 'colsample_bytree': 0.8}\n",
      "\n",
      "Advanced XGBoost with feature engineering...\n",
      "Advanced XGBoost R² Score: 0.848310\n",
      "\n",
      "Ensemble XGBoost...\n",
      "Training ensemble model 1/5\n",
      "Training ensemble model 2/5\n",
      "Training ensemble model 3/5\n",
      "Training ensemble model 4/5\n",
      "Training ensemble model 5/5\n",
      "Ensemble XGBoost R² Score: 0.854406\n",
      "\n",
      "Cross-validation scores...\n",
      "CV R² Score: 0.851676 (+/- 0.018981)\n",
      "\n",
      "Top 10 feature importances:\n",
      "         feature  importance\n",
      "0      feature_0    0.336188\n",
      "8   feature_0_sq    0.285399\n",
      "5      feature_5    0.071422\n",
      "7      feature_7    0.053860\n",
      "10   feature_0_1    0.052052\n",
      "6      feature_6    0.051723\n",
      "14     ratio_2_3    0.024971\n",
      "2      feature_2    0.021791\n",
      "1      feature_1    0.021091\n",
      "11   feature_2_3    0.015154\n",
      "\n",
      "==================================================\n",
      "RESULTS SUMMARY\n",
      "==================================================\n",
      "Basic XGBoost R²:     0.849273\n",
      "Tuned XGBoost R²:     0.852921\n",
      "Advanced XGBoost R²:  0.848310\n",
      "Ensemble XGBoost R²:  0.854406\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Basic XGBoost\n",
    "print(\"Training Basic XGBoost...\")\n",
    "xgb_basic = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_basic.fit(X_train, y_train)\n",
    "y_pred_basic = xgb_basic.predict(X_test)\n",
    "r2_basic = r2_score(y_test, y_pred_basic)\n",
    "print(f\"Basic XGBoost R² Score: {r2_basic:.6f}\")\n",
    "\n",
    "# Hyperparameter tuning with RandomizedSearchCV\n",
    "print(\"\\nHyperparameter tuning...\")\n",
    "param_dist = {\n",
    "    'n_estimators': [500, 1000, 1500, 2000],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1.0, 2.0],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "xgb_random = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_random, \n",
    "    param_dist, \n",
    "    n_iter=50, \n",
    "    scoring='r2', \n",
    "    cv=5, \n",
    "    verbose=1, \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "best_xgb = random_search.best_estimator_\n",
    "y_pred_tuned = best_xgb.predict(X_test)\n",
    "r2_tuned = r2_score(y_test, y_pred_tuned)\n",
    "print(f\"Tuned XGBoost R² Score: {r2_tuned:.6f}\")\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "\n",
    "# Advanced XGBoost with feature engineering\n",
    "print(\"\\nAdvanced XGBoost with feature engineering...\")\n",
    "\n",
    "# Create additional features\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "\n",
    "# Add polynomial features\n",
    "df['feature_0_sq'] = df['feature_0'] ** 2\n",
    "df['feature_1_sq'] = df['feature_1'] ** 2\n",
    "df['feature_0_1'] = df['feature_0'] * df['feature_1']\n",
    "df['feature_2_3'] = df['feature_2'] * df['feature_3']\n",
    "df['feature_4_5'] = df['feature_4'] * df['feature_5']\n",
    "\n",
    "# Add ratios\n",
    "df['ratio_0_1'] = df['feature_0'] / (df['feature_1'] + 1e-8)\n",
    "df['ratio_2_3'] = df['feature_2'] / (df['feature_3'] + 1e-8)\n",
    "\n",
    "X_enhanced = df.values\n",
    "X_train_enh, X_test_enh, y_train_enh, y_test_enh = train_test_split(\n",
    "    X_enhanced, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_enh_scaled = scaler.fit_transform(X_train_enh)\n",
    "X_test_enh_scaled = scaler.transform(X_test_enh)\n",
    "\n",
    "xgb_advanced = xgb.XGBRegressor(\n",
    "    n_estimators=2000,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_advanced.fit(X_train_enh_scaled, y_train_enh)\n",
    "y_pred_advanced = xgb_advanced.predict(X_test_enh_scaled)\n",
    "r2_advanced = r2_score(y_test_enh, y_pred_advanced)\n",
    "print(f\"Advanced XGBoost R² Score: {r2_advanced:.6f}\")\n",
    "\n",
    "# Ensemble of XGBoost models\n",
    "print(\"\\nEnsemble XGBoost...\")\n",
    "\n",
    "class XGBoostEnsemble:\n",
    "    def __init__(self, n_models=5):\n",
    "        self.models = []\n",
    "        self.n_models = n_models\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Different configurations for ensemble diversity\n",
    "        configs = [\n",
    "            {'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8},\n",
    "            {'max_depth': 7, 'learning_rate': 0.05, 'subsample': 0.9, 'colsample_bytree': 0.7},\n",
    "            {'max_depth': 5, 'learning_rate': 0.15, 'subsample': 0.7, 'colsample_bytree': 0.9},\n",
    "            {'max_depth': 8, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.85},\n",
    "            {'max_depth': 4, 'learning_rate': 0.2, 'subsample': 0.75, 'colsample_bytree': 0.75}\n",
    "        ]\n",
    "        \n",
    "        for i in range(self.n_models):\n",
    "            print(f\"Training ensemble model {i+1}/{self.n_models}\")\n",
    "            config = configs[i]\n",
    "            \n",
    "            model = xgb.XGBRegressor(\n",
    "                n_estimators=1500,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                min_child_weight=3,\n",
    "                random_state=42 + i,\n",
    "                n_jobs=-1,\n",
    "                **config\n",
    "            )\n",
    "            \n",
    "            model.fit(X, y)\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        return np.mean(predictions, axis=0)\n",
    "\n",
    "ensemble = XGBoostEnsemble(n_models=5)\n",
    "ensemble.fit(X_train_enh_scaled, y_train_enh)\n",
    "y_pred_ensemble = ensemble.predict(X_test_enh_scaled)\n",
    "r2_ensemble = r2_score(y_test_enh, y_pred_ensemble)\n",
    "print(f\"Ensemble XGBoost R² Score: {r2_ensemble:.6f}\")\n",
    "\n",
    "# Cross-validation for more robust evaluation\n",
    "print(\"\\nCross-validation scores...\")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(best_xgb, X_train, y_train, cv=10, scoring='r2')\n",
    "print(f\"CV R² Score: {cv_scores.mean():.6f} (+/- {cv_scores.std() * 2:.6f})\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nTop 10 feature importances:\")\n",
    "feature_names = [f'feature_{i}' for i in range(8)] + ['feature_0_sq', 'feature_1_sq', 'feature_0_1', 'feature_2_3', 'feature_4_5', 'ratio_0_1', 'ratio_2_3']\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': xgb_advanced.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Basic XGBoost R²:     {r2_basic:.6f}\")\n",
    "print(f\"Tuned XGBoost R²:     {r2_tuned:.6f}\")\n",
    "print(f\"Advanced XGBoost R²:  {r2_advanced:.6f}\")\n",
    "print(f\"Ensemble XGBoost R²:  {r2_ensemble:.6f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66420c26-2c6d-4d0c-a519-c5968484c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TabNet-like model...\n",
      "Training Neural Decision Tree...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104b6ee80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 77, in __del__\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 86, in _stop\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 111, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x104b56e80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 77, in __del__\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 86, in _stop\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 111, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10496ee80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 77, in __del__\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 86, in _stop\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 111, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x10686ee80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 77, in __del__\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 86, in _stop\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 111, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1036f2e80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 77, in __del__\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 86, in _stop\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 111, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x12058ae80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 77, in __del__\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 86, in _stop\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 111, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x103ef2e80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 77, in __del__\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 86, in _stop\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 111, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1034c2e80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 77, in __del__\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 86, in _stop\n",
      "  File \"/Users/calld-admin/.pyenv/versions/3.12.10/lib/python3.12/multiprocessing/resource_tracker.py\", line 111, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tree-inspired Network...\n",
      "Training Feature Engineering Net...\n",
      "Training Neural Gradient Boosting...\n",
      "Estimator 0, Loss: 0.324452\n",
      "Estimator 10, Loss: 0.320138\n",
      "Estimator 20, Loss: 0.249264\n",
      "\n",
      "==================================================\n",
      "RESULTS COMPARISON\n",
      "==================================================\n",
      "XGBoost baseline:           0.849273\n",
      "TabNet-like              : -25.061192\n",
      "Neural Decision Tree     : 0.790182\n",
      "Tree-inspired Net        : 0.788188\n",
      "Feature Engineering Net  : 0.792173\n",
      "Neural Gradient Boosting : -2.542184\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Fixed TabNet-inspired architecture\n",
    "class FeatureTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.bn = nn.BatchNorm1d(output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.fc(x))\n",
    "\n",
    "class AttentiveTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.bn = nn.BatchNorm1d(output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.fc(x))\n",
    "\n",
    "class TabNetLike(nn.Module):\n",
    "    def __init__(self, input_dim, n_d=64, n_a=64, n_steps=3):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        \n",
    "        # Feature transformer\n",
    "        self.initial_bn = nn.BatchNorm1d(input_dim)\n",
    "        \n",
    "        # Attention transformers for each step - FIXED DIMENSIONS\n",
    "        self.att_transformers = nn.ModuleList([\n",
    "            AttentiveTransformer(input_dim, input_dim) for _ in range(n_steps)\n",
    "        ])\n",
    "        \n",
    "        # Feature transformers for each step\n",
    "        self.feat_transformers = nn.ModuleList([\n",
    "            FeatureTransformer(input_dim, n_d + n_a) for _ in range(n_steps)\n",
    "        ])\n",
    "        \n",
    "        # Final output layer\n",
    "        self.final_mapping = nn.Linear(n_d, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.initial_bn(x)\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "        # Initialize prior\n",
    "        prior = torch.ones(bs, self.input_dim).to(x.device)\n",
    "        \n",
    "        # Aggregate decision output\n",
    "        output_agg = torch.zeros(bs, self.n_d).to(x.device)\n",
    "        \n",
    "        for step in range(self.n_steps):\n",
    "            # Attentive transformer\n",
    "            mask_values = self.att_transformers[step](prior)\n",
    "            mask_values = torch.softmax(mask_values, dim=-1)\n",
    "            \n",
    "            # Feature selection\n",
    "            masked_x = mask_values * x\n",
    "            \n",
    "            # Feature transformer\n",
    "            out = self.feat_transformers[step](masked_x)\n",
    "            d = torch.relu(out[:, :self.n_d])\n",
    "            \n",
    "            # Aggregate\n",
    "            output_agg += d\n",
    "            \n",
    "            # Update prior\n",
    "            prior = mask_values\n",
    "            \n",
    "        return self.final_mapping(output_agg)\n",
    "\n",
    "# 2. Simplified Neural Decision Tree\n",
    "class NeuralDecisionTree(nn.Module):\n",
    "    def __init__(self, input_dim, n_trees=10, tree_depth=4):\n",
    "        super().__init__()\n",
    "        self.n_trees = n_trees\n",
    "        self.tree_depth = tree_depth\n",
    "        \n",
    "        # Multiple trees\n",
    "        self.trees = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 1)\n",
    "            ) for _ in range(n_trees)\n",
    "        ])\n",
    "        \n",
    "        # Tree weights\n",
    "        self.tree_weights = nn.Parameter(torch.ones(n_trees) / n_trees)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        tree_outputs = []\n",
    "        for tree in self.trees:\n",
    "            tree_outputs.append(tree(x))\n",
    "        \n",
    "        # Weighted combination\n",
    "        tree_outputs = torch.stack(tree_outputs, dim=-1)\n",
    "        weights = torch.softmax(self.tree_weights, dim=0)\n",
    "        output = torch.sum(tree_outputs * weights, dim=-1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 3. Simplified Neural Gradient Boosting\n",
    "class WeakNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class NeuralGradientBoosting:\n",
    "    def __init__(self, input_dim, n_estimators=50, learning_rate=0.1):\n",
    "        self.input_dim = input_dim\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "        y_tensor = torch.FloatTensor(y).reshape(-1, 1).to(self.device)\n",
    "        \n",
    "        # Initialize prediction with mean\n",
    "        pred = torch.full_like(y_tensor, y_tensor.mean())\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Calculate residuals\n",
    "            residuals = y_tensor - pred\n",
    "            \n",
    "            # Train weak learner on residuals\n",
    "            model = WeakNeuralNetwork(self.input_dim).to(self.device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "            criterion = nn.MSELoss()\n",
    "            \n",
    "            # Quick training\n",
    "            dataset = TensorDataset(X_tensor, residuals)\n",
    "            loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "            \n",
    "            model.train()\n",
    "            for epoch in range(20):\n",
    "                for batch_x, batch_y in loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(batch_x)\n",
    "                    loss = criterion(output, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # Add to ensemble\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred += self.learning_rate * model(X_tensor)\n",
    "            \n",
    "            self.models.append(model)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"Estimator {i}, Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Start with mean from training\n",
    "            pred = torch.zeros(X.shape[0], 1).to(self.device)\n",
    "            for model in self.models:\n",
    "                model.eval()\n",
    "                pred += self.learning_rate * model(X_tensor)\n",
    "        \n",
    "        return pred.cpu().numpy().flatten()\n",
    "\n",
    "# 4. Feature Engineering Network\n",
    "class FeatureEngineeringNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Feature interaction layers\n",
    "        self.pairwise_net = nn.Sequential(\n",
    "            nn.Linear(input_dim * input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2)\n",
    "        )\n",
    "        \n",
    "        # Original features processor\n",
    "        self.original_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Main network\n",
    "        self.main_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.BatchNorm1d(hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
    "            nn.BatchNorm1d(hidden_dim//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim//4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Create pairwise interactions\n",
    "        x_expanded = x.unsqueeze(2)  # (batch, features, 1)\n",
    "        x_T = x.unsqueeze(1)        # (batch, 1, features)\n",
    "        interactions = (x_expanded * x_T).view(x.shape[0], -1)  # (batch, features^2)\n",
    "        \n",
    "        # Process interactions\n",
    "        interaction_features = self.pairwise_net(interactions)\n",
    "        \n",
    "        # Process original features\n",
    "        original_features = self.original_net(x)\n",
    "        \n",
    "        # Combine and process\n",
    "        combined = torch.cat([interaction_features, original_features], dim=1)\n",
    "        return self.main_net(combined)\n",
    "\n",
    "# 5. Tree-inspired Network with explicit splits\n",
    "class TreeInspiredNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_leaves=64):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_leaves = n_leaves\n",
    "        \n",
    "        # Feature selection for splits\n",
    "        self.feature_selectors = nn.ModuleList([\n",
    "            nn.Linear(input_dim, 1) for _ in range(n_leaves)\n",
    "        ])\n",
    "        \n",
    "        # Threshold learners\n",
    "        self.thresholds = nn.Parameter(torch.randn(n_leaves))\n",
    "        \n",
    "        # Leaf predictors\n",
    "        self.leaf_predictors = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 1)\n",
    "            ) for _ in range(n_leaves)\n",
    "        ])\n",
    "        \n",
    "        # Gating network\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_leaves),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get gate weights\n",
    "        gate_weights = self.gate(x)\n",
    "        \n",
    "        # Get predictions from each leaf\n",
    "        leaf_outputs = []\n",
    "        for i, predictor in enumerate(self.leaf_predictors):\n",
    "            leaf_outputs.append(predictor(x))\n",
    "        \n",
    "        # Stack and weight\n",
    "        leaf_outputs = torch.stack(leaf_outputs, dim=-1).squeeze(1)\n",
    "        output = torch.sum(gate_weights * leaf_outputs, dim=1, keepdim=True)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test all approaches\n",
    "def test_approaches():\n",
    "    # Load data\n",
    "    X, y = fetch_california_housing(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    results = {}\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    \n",
    "    # Common training setup\n",
    "    criterion = nn.MSELoss()\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train_scaled), torch.FloatTensor(y_train))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    \n",
    "    # 1. TabNet-like\n",
    "    print(\"Training TabNet-like model...\")\n",
    "    model1 = TabNetLike(X.shape[1]).to(device)\n",
    "    optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
    "    \n",
    "    model1.train()\n",
    "    for epoch in range(100):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer1.zero_grad()\n",
    "            pred = model1(batch_x).squeeze()\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer1.step()\n",
    "    \n",
    "    model1.eval()\n",
    "    with torch.no_grad():\n",
    "        test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "        pred1 = model1(test_tensor).squeeze().cpu().numpy()\n",
    "    results['TabNet-like'] = r2_score(y_test, pred1)\n",
    "    \n",
    "    # 2. Neural Decision Tree\n",
    "    print(\"Training Neural Decision Tree...\")\n",
    "    model2 = NeuralDecisionTree(X.shape[1]).to(device)\n",
    "    optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "    \n",
    "    model2.train()\n",
    "    for epoch in range(100):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer2.zero_grad()\n",
    "            pred = model2(batch_x).squeeze()\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "    \n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        pred2 = model2(test_tensor).squeeze().cpu().numpy()\n",
    "    results['Neural Decision Tree'] = r2_score(y_test, pred2)\n",
    "    \n",
    "    # 3. Tree-inspired Network\n",
    "    print(\"Training Tree-inspired Network...\")\n",
    "    model3 = TreeInspiredNet(X.shape[1]).to(device)\n",
    "    optimizer3 = optim.Adam(model3.parameters(), lr=0.001)\n",
    "    \n",
    "    model3.train()\n",
    "    for epoch in range(100):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer3.zero_grad()\n",
    "            pred = model3(batch_x).squeeze()\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer3.step()\n",
    "    \n",
    "    model3.eval()\n",
    "    with torch.no_grad():\n",
    "        pred3 = model3(test_tensor).squeeze().cpu().numpy()\n",
    "    results['Tree-inspired Net'] = r2_score(y_test, pred3)\n",
    "    \n",
    "    # 4. Feature Engineering Net\n",
    "    print(\"Training Feature Engineering Net...\")\n",
    "    model4 = FeatureEngineeringNet(X.shape[1]).to(device)\n",
    "    optimizer4 = optim.Adam(model4.parameters(), lr=0.0005)\n",
    "    \n",
    "    model4.train()\n",
    "    for epoch in range(150):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer4.zero_grad()\n",
    "            pred = model4(batch_x).squeeze()\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer4.step()\n",
    "    \n",
    "    model4.eval()\n",
    "    with torch.no_grad():\n",
    "        pred4 = model4(test_tensor).squeeze().cpu().numpy()\n",
    "    results['Feature Engineering Net'] = r2_score(y_test, pred4)\n",
    "    \n",
    "    # 5. Neural Gradient Boosting\n",
    "    print(\"Training Neural Gradient Boosting...\")\n",
    "    ngb = NeuralGradientBoosting(X.shape[1], n_estimators=30)\n",
    "    ngb.fit(X_train_scaled, y_train)\n",
    "    pred5 = ngb.predict(X_test_scaled)\n",
    "    results['Neural Gradient Boosting'] = r2_score(y_test, pred5)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RESULTS COMPARISON\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"XGBoost baseline:           0.849273\")\n",
    "    for name, score in results.items():\n",
    "        print(f\"{name:<25}: {score:.6f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "test_approaches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b2225f1-c748-46e9-bc61-aaf89fca0a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (20640, 8)\n",
      "Processed data shape: (16512, 8)\n",
      "\n",
      "1. Training Simple Deep Network...\n",
      "\n",
      "2. Training Wide & Deep Network...\n",
      "\n",
      "3. Training Tabular ResNet...\n",
      "\n",
      "4. Training Heterogeneous Ensemble...\n",
      "Training SimpleDeep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calld-admin/Documents/MCP/notebooks/env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WideDeep...\n",
      "Training ResNet...\n",
      "Training SimpleDeep2...\n",
      "Training ResNet2...\n",
      "\n",
      "============================================================\n",
      "NEURAL NETWORK RESULTS (with proper training)\n",
      "============================================================\n",
      "XGBoost baseline:                    0.849273\n",
      "------------------------------------------------------------\n",
      "Simple Deep Net               : 0.815544 (gap: 0.033729)\n",
      "Wide & Deep                   : 0.818977 (gap: 0.030296)\n",
      "Tabular ResNet                : 0.800976 (gap: 0.048297)\n",
      "Ensemble                      : 0.827754 (gap: 0.021519)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Simple but Effective Deep Network\n",
    "class SimpleDeepNet(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input layer\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Hidden layers\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout/2),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Output\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.BatchNorm1d):\n",
    "            nn.init.constant_(module.weight, 1)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 2. Wide & Deep Network (Google's approach)\n",
    "class WideAndDeep(nn.Module):\n",
    "    def __init__(self, input_dim, wide_dim=None, deep_hidden=[512, 256, 128]):\n",
    "        super().__init__()\n",
    "        \n",
    "        if wide_dim is None:\n",
    "            wide_dim = input_dim\n",
    "        \n",
    "        # Wide part (linear model)\n",
    "        self.wide = nn.Linear(wide_dim, 1)\n",
    "        \n",
    "        # Deep part\n",
    "        deep_layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in deep_hidden:\n",
    "            deep_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        deep_layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.deep = nn.Sequential(*deep_layers)\n",
    "        \n",
    "        # Combination\n",
    "        self.output_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        wide_out = self.wide(x)\n",
    "        deep_out = self.deep(x)\n",
    "        return wide_out + deep_out + self.output_bias\n",
    "\n",
    "# 3. Residual Network for Tabular Data\n",
    "class TabularResNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, n_blocks=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            self._make_block(hidden_dim) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _make_block(self, dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            residual = x\n",
    "            x = block(x)\n",
    "            x = torch.relu(x + residual)  # Residual connection\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "# 4. Ensemble of Different Architectures\n",
    "class HeterogeneousEnsemble:\n",
    "    def __init__(self, input_dim):\n",
    "        self.models = []\n",
    "        self.input_dim = input_dim\n",
    "        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Convert to tensors\n",
    "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "        y_tensor = torch.FloatTensor(y).to(self.device)\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        train_loader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "        \n",
    "        # Model configurations\n",
    "        model_configs = [\n",
    "            ('SimpleDeep', SimpleDeepNet(self.input_dim, dropout=0.2)),\n",
    "            ('WideDeep', WideAndDeep(self.input_dim)),\n",
    "            ('ResNet', TabularResNet(self.input_dim, hidden_dim=512, n_blocks=3)),\n",
    "            ('SimpleDeep2', SimpleDeepNet(self.input_dim, dropout=0.4)),\n",
    "            ('ResNet2', TabularResNet(self.input_dim, hidden_dim=256, n_blocks=6))\n",
    "        ]\n",
    "        \n",
    "        for name, model in model_configs:\n",
    "            print(f\"Training {name}...\")\n",
    "            model = model.to(self.device)\n",
    "            \n",
    "            # Different optimizers and learning rates\n",
    "            if 'Wide' in name:\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "            else:\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "            \n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, patience=10, factor=0.5, verbose=False\n",
    "            )\n",
    "            \n",
    "            criterion = nn.MSELoss()\n",
    "            best_loss = float('inf')\n",
    "            patience = 0\n",
    "            \n",
    "            model.train()\n",
    "            for epoch in range(200):\n",
    "                epoch_loss = 0\n",
    "                for batch_x, batch_y in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = model(batch_x).squeeze()\n",
    "                    loss = criterion(pred, batch_y)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    epoch_loss += loss.item()\n",
    "                \n",
    "                avg_loss = epoch_loss / len(train_loader)\n",
    "                scheduler.step(avg_loss)\n",
    "                \n",
    "                # Early stopping\n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    patience = 0\n",
    "                else:\n",
    "                    patience += 1\n",
    "                    if patience >= 20:\n",
    "                        break\n",
    "            \n",
    "            model.eval()\n",
    "            self.models.append((name, model))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for name, model in self.models:\n",
    "                model.eval()\n",
    "                pred = model(X_tensor).squeeze().cpu().numpy()\n",
    "                predictions.append(pred)\n",
    "        \n",
    "        # Simple average ensemble\n",
    "        return np.mean(predictions, axis=0)\n",
    "\n",
    "# Enhanced preprocessing\n",
    "def advanced_preprocessing(X_train, X_test):\n",
    "    \"\"\"Apply advanced preprocessing that neural networks benefit from\"\"\"\n",
    "    \n",
    "    # 1. Quantile transformation (makes features more Gaussian)\n",
    "    qt = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "    X_train_qt = qt.fit_transform(X_train)\n",
    "    X_test_qt = qt.transform(X_test)\n",
    "    \n",
    "    # 2. Standard scaling after quantile transform\n",
    "    scaler = StandardScaler()\n",
    "    X_train_final = scaler.fit_transform(X_train_qt)\n",
    "    X_test_final = scaler.transform(X_test_qt)\n",
    "    \n",
    "    return X_train_final, X_test_final\n",
    "\n",
    "# Main training function\n",
    "def train_neural_networks():\n",
    "    # Load data\n",
    "    X, y = fetch_california_housing(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Enhanced preprocessing\n",
    "    X_train_processed, X_test_processed = advanced_preprocessing(X_train, X_test)\n",
    "    \n",
    "    print(\"Original data shape:\", X.shape)\n",
    "    print(\"Processed data shape:\", X_train_processed.shape)\n",
    "    \n",
    "    results = {}\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    \n",
    "    # Common training setup\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # 1. Simple Deep Network\n",
    "    print(\"\\n1. Training Simple Deep Network...\")\n",
    "    model1 = SimpleDeepNet(X.shape[1]).to(device)\n",
    "    optimizer1 = optim.AdamW(model1.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler1 = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer1, max_lr=0.01, epochs=300, \n",
    "        steps_per_epoch=len(X_train_processed)//512 + 1\n",
    "    )\n",
    "    \n",
    "    train_dataset1 = TensorDataset(\n",
    "        torch.FloatTensor(X_train_processed), \n",
    "        torch.FloatTensor(y_train)\n",
    "    )\n",
    "    train_loader1 = DataLoader(train_dataset1, batch_size=512, shuffle=True)\n",
    "    \n",
    "    model1.train()\n",
    "    for epoch in range(300):\n",
    "        for batch_x, batch_y in train_loader1:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer1.zero_grad()\n",
    "            pred = model1(batch_x).squeeze()\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model1.parameters(), max_norm=1.0)\n",
    "            optimizer1.step()\n",
    "            scheduler1.step()\n",
    "    \n",
    "    model1.eval()\n",
    "    with torch.no_grad():\n",
    "        test_tensor = torch.FloatTensor(X_test_processed).to(device)\n",
    "        pred1 = model1(test_tensor).squeeze().cpu().numpy()\n",
    "    results['Simple Deep Net'] = r2_score(y_test, pred1)\n",
    "    \n",
    "    # 2. Wide & Deep\n",
    "    print(\"\\n2. Training Wide & Deep Network...\")\n",
    "    model2 = WideAndDeep(X.shape[1]).to(device)\n",
    "    optimizer2 = optim.Adam(model2.parameters(), lr=0.001, weight_decay=0.001)\n",
    "    \n",
    "    train_dataset2 = TensorDataset(\n",
    "        torch.FloatTensor(X_train_processed), \n",
    "        torch.FloatTensor(y_train)\n",
    "    )\n",
    "    train_loader2 = DataLoader(train_dataset2, batch_size=512, shuffle=True)\n",
    "    \n",
    "    model2.train()\n",
    "    for epoch in range(200):\n",
    "        for batch_x, batch_y in train_loader2:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer2.zero_grad()\n",
    "            pred = model2(batch_x).squeeze()\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "    \n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        pred2 = model2(test_tensor).squeeze().cpu().numpy()\n",
    "    results['Wide & Deep'] = r2_score(y_test, pred2)\n",
    "    \n",
    "    # 3. Tabular ResNet\n",
    "    print(\"\\n3. Training Tabular ResNet...\")\n",
    "    model3 = TabularResNet(X.shape[1]).to(device)\n",
    "    optimizer3 = optim.Adam(model3.parameters(), lr=0.001, weight_decay=0.001)\n",
    "    \n",
    "    model3.train()\n",
    "    for epoch in range(200):\n",
    "        for batch_x, batch_y in train_loader2:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer3.zero_grad()\n",
    "            pred = model3(batch_x).squeeze()\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model3.parameters(), max_norm=1.0)\n",
    "            optimizer3.step()\n",
    "    \n",
    "    model3.eval()\n",
    "    with torch.no_grad():\n",
    "        pred3 = model3(test_tensor).squeeze().cpu().numpy()\n",
    "    results['Tabular ResNet'] = r2_score(y_test, pred3)\n",
    "    \n",
    "    # 4. Ensemble\n",
    "    print(\"\\n4. Training Heterogeneous Ensemble...\")\n",
    "    ensemble = HeterogeneousEnsemble(X.shape[1])\n",
    "    ensemble.fit(X_train_processed, y_train)\n",
    "    pred_ensemble = ensemble.predict(X_test_processed)\n",
    "    results['Ensemble'] = r2_score(y_test, pred_ensemble)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NEURAL NETWORK RESULTS (with proper training)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"XGBoost baseline:                    0.849273\")\n",
    "    print(\"-\"*60)\n",
    "    for name, score in results.items():\n",
    "        gap = abs(0.849273 - score)\n",
    "        print(f\"{name:<30}: {score:.6f} (gap: {gap:.6f})\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the comparison\n",
    "results = train_neural_networks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffe11af9-1e51-450b-96c7-805ce5da3ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced features...\n",
      "Original features: 8\n",
      "Enhanced features: 111\n",
      "Preprocessing...\n",
      "Training advanced ensemble...\n",
      "Training DeepNet1...\n",
      "  Epoch 0, Loss: 3.406161\n",
      "  Epoch 50, Loss: 0.322034\n",
      "  Epoch 100, Loss: 0.300962\n",
      "  Epoch 150, Loss: 0.276026\n",
      "Training WideDeep...\n",
      "  Epoch 0, Loss: 2.396959\n",
      "  Epoch 50, Loss: 0.279442\n",
      "  Epoch 100, Loss: 0.257549\n",
      "  Epoch 150, Loss: 0.207181\n",
      "Training ResNet...\n",
      "  Epoch 0, Loss: 0.864335\n",
      "  Epoch 50, Loss: 0.305222\n",
      "  Epoch 100, Loss: 0.280137\n",
      "  Epoch 150, Loss: 0.258561\n",
      "Training Attention...\n",
      "  Epoch 0, Loss: 2.297671\n",
      "  Epoch 50, Loss: 0.191397\n",
      "  Epoch 100, Loss: 0.059614\n",
      "Training DeepNet2...\n",
      "  Epoch 0, Loss: 4.631473\n",
      "  Epoch 50, Loss: 0.330715\n",
      "  Epoch 100, Loss: 0.314646\n",
      "  Epoch 150, Loss: 0.291762\n",
      "Making predictions...\n",
      "\n",
      "============================================================\n",
      "ULTIMATE NEURAL NETWORK RESULTS\n",
      "============================================================\n",
      "XGBoost baseline:                    0.849273\n",
      "Ultimate Neural Network:             0.817338\n",
      "Gap closed:                          0.031935\n",
      "📈 Room for improvement, but still decent!\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8173384968370929"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fixed Advanced Feature Engineering\n",
    "def create_advanced_features(X_train, X_test):\n",
    "    \"\"\"Create XGBoost-like feature interactions consistently across train/test\"\"\"\n",
    "    \n",
    "    # Combine for consistent feature engineering\n",
    "    X_combined = np.vstack([X_train, X_test])\n",
    "    n_train = len(X_train)\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    # Original features\n",
    "    features_list.append(X_combined)\n",
    "    \n",
    "    # Polynomial features (degree 2, interactions only)\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    poly_features = poly.fit_transform(X_combined)\n",
    "    # Only new features (skip original ones)\n",
    "    features_list.append(poly_features[:, X_combined.shape[1]:])\n",
    "    \n",
    "    # Log transformations (for positive features)\n",
    "    for i in range(X_combined.shape[1]):\n",
    "        if np.all(X_combined[:, i] > 0):\n",
    "            log_feat = np.log1p(X_combined[:, i:i+1])\n",
    "            features_list.append(log_feat)\n",
    "    \n",
    "    # Ratios between features\n",
    "    for i in range(X_combined.shape[1]):\n",
    "        for j in range(i+1, X_combined.shape[1]):\n",
    "            if np.all(X_combined[:, j] != 0):\n",
    "                ratio = X_combined[:, i:i+1] / (X_combined[:, j:j+1] + 1e-8)\n",
    "                features_list.append(ratio)\n",
    "    \n",
    "    # Binning features (using quantiles from combined data)\n",
    "    for i in range(X_combined.shape[1]):\n",
    "        # Use quantiles for consistent binning\n",
    "        quantiles = np.percentile(X_combined[:, i], [0, 20, 40, 60, 80, 100])\n",
    "        quantiles = np.unique(quantiles)  # Remove duplicates\n",
    "        \n",
    "        if len(quantiles) > 1:\n",
    "            binned = np.digitize(X_combined[:, i], quantiles[1:-1])\n",
    "            # One-hot encode\n",
    "            n_bins = len(quantiles) - 1\n",
    "            binned_onehot = np.eye(n_bins)[binned]\n",
    "            features_list.append(binned_onehot)\n",
    "    \n",
    "    # Combine all features\n",
    "    X_enhanced = np.hstack(features_list).astype(np.float32)\n",
    "    \n",
    "    # Split back\n",
    "    X_train_enhanced = X_enhanced[:n_train]\n",
    "    X_test_enhanced = X_enhanced[n_train:]\n",
    "    \n",
    "    return X_train_enhanced, X_test_enhanced\n",
    "\n",
    "# Self-Attention Network for Tabular Data\n",
    "class SelfAttentionTabular(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=128, num_heads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature embedding\n",
    "        self.feature_embed = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embed_dim * 2, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Output head\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(embed_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embed features\n",
    "        x = self.feature_embed(x)  # (batch, embed_dim)\n",
    "        \n",
    "        # Add position-like encoding (feature importance)\n",
    "        x = x.unsqueeze(1)  # (batch, 1, embed_dim)\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_out, _ = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        \n",
    "        # Output\n",
    "        x = x.squeeze(1)  # (batch, embed_dim)\n",
    "        return self.output(x)\n",
    "\n",
    "# Swish activation\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "# Simplified but Effective Ensemble\n",
    "class AdvancedEnsemble:\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        self.models = []\n",
    "        \n",
    "    def create_models(self):\n",
    "        \"\"\"Create diverse models\"\"\"\n",
    "        \n",
    "        # Model 1: Deep Network with GELU\n",
    "        model1 = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Model 2: Wide & Deep\n",
    "        class WideDeep(nn.Module):\n",
    "            def __init__(self, input_dim):\n",
    "                super().__init__()\n",
    "                self.wide = nn.Linear(input_dim, 1)\n",
    "                self.deep = nn.Sequential(\n",
    "                    nn.Linear(input_dim, 512),\n",
    "                    nn.BatchNorm1d(512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.BatchNorm1d(256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 1)\n",
    "                )\n",
    "                \n",
    "            def forward(self, x):\n",
    "                return self.wide(x) + self.deep(x)\n",
    "        \n",
    "        model2 = WideDeep(self.input_dim)\n",
    "        \n",
    "        # Model 3: ResNet-style (FIXED GELU)\n",
    "        class ResBlock(nn.Module):\n",
    "            def __init__(self, dim):\n",
    "                super().__init__()\n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(dim, dim),\n",
    "                    nn.BatchNorm1d(dim),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(0.1),\n",
    "                    nn.Linear(dim, dim),\n",
    "                    nn.BatchNorm1d(dim)\n",
    "                )\n",
    "                \n",
    "            def forward(self, x):\n",
    "                return F.gelu(x + self.net(x))  # Fixed: use F.gelu\n",
    "        \n",
    "        class ResNet(nn.Module):\n",
    "            def __init__(self, input_dim):\n",
    "                super().__init__()\n",
    "                self.input_proj = nn.Sequential(\n",
    "                    nn.Linear(input_dim, 512),\n",
    "                    nn.BatchNorm1d(512),\n",
    "                    nn.GELU()\n",
    "                )\n",
    "                self.blocks = nn.Sequential(*[ResBlock(512) for _ in range(6)])\n",
    "                self.output = nn.Sequential(\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(256, 1)\n",
    "                )\n",
    "                \n",
    "            def forward(self, x):\n",
    "                x = self.input_proj(x)\n",
    "                x = self.blocks(x)\n",
    "                return self.output(x)\n",
    "        \n",
    "        model3 = ResNet(self.input_dim)\n",
    "        \n",
    "        # Model 4: Attention-based\n",
    "        model4 = SelfAttentionTabular(self.input_dim, embed_dim=256, num_heads=8)\n",
    "        \n",
    "        # Model 5: Another deep net with Swish activation\n",
    "        model5 = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            Swish(),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(768, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            Swish(),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(384, 192),\n",
    "            nn.BatchNorm1d(192),\n",
    "            Swish(),\n",
    "            nn.Dropout(0.15),\n",
    "            \n",
    "            nn.Linear(192, 1)\n",
    "        )\n",
    "        \n",
    "        return [\n",
    "            ('DeepNet1', model1),\n",
    "            ('WideDeep', model2),\n",
    "            ('ResNet', model3),\n",
    "            ('Attention', model4),\n",
    "            ('DeepNet2', model5)\n",
    "        ]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        models = self.create_models()\n",
    "        \n",
    "        for name, model in models:\n",
    "            print(f\"Training {name}...\")\n",
    "            model = model.to(self.device)\n",
    "            \n",
    "            # Different training configurations\n",
    "            if 'Attention' in name:\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.01)\n",
    "                epochs = 150\n",
    "            elif 'ResNet' in name:\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.005)\n",
    "                epochs = 200\n",
    "            else:\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "                epochs = 200\n",
    "            \n",
    "            # Learning rate scheduler\n",
    "            scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "                optimizer, max_lr=optimizer.param_groups[0]['lr'] * 3,\n",
    "                epochs=epochs, steps_per_epoch=len(X)//256 + 1\n",
    "            )\n",
    "            \n",
    "            criterion = nn.MSELoss()\n",
    "            \n",
    "            # Create data loader\n",
    "            dataset = TensorDataset(torch.FloatTensor(X), torch.FloatTensor(y))\n",
    "            loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "            \n",
    "            # Training loop\n",
    "            model.train()\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for batch_x, batch_y in loader:\n",
    "                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    pred = model(batch_x).squeeze()\n",
    "                    loss = criterion(pred, batch_y)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    epoch_loss += loss.item()\n",
    "                \n",
    "                if epoch % 50 == 0:\n",
    "                    avg_loss = epoch_loss / len(loader)\n",
    "                    print(f\"  Epoch {epoch}, Loss: {avg_loss:.6f}\")\n",
    "            \n",
    "            model.eval()\n",
    "            self.models.append((name, model))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "            \n",
    "            for name, model in self.models:\n",
    "                model.eval()\n",
    "                pred = model(X_tensor).squeeze().cpu().numpy()\n",
    "                predictions.append(pred)\n",
    "        \n",
    "        # Weighted average (you could learn these weights too)\n",
    "        weights = np.array([0.25, 0.20, 0.20, 0.15, 0.20])  # Slightly prefer deep nets\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        final_pred = np.average(predictions, axis=0, weights=weights)\n",
    "        \n",
    "        return final_pred\n",
    "\n",
    "# Ultimate approach with all fixes\n",
    "def ultimate_neural_network_approach():\n",
    "    # Load data\n",
    "    X, y = fetch_california_housing(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(\"Creating advanced features...\")\n",
    "    # Create enhanced features (fixed version)\n",
    "    X_train_enhanced, X_test_enhanced = create_advanced_features(X_train, X_test)\n",
    "    \n",
    "    print(f\"Original features: {X.shape[1]}\")\n",
    "    print(f\"Enhanced features: {X_train_enhanced.shape[1]}\")\n",
    "    \n",
    "    # Advanced preprocessing\n",
    "    print(\"Preprocessing...\")\n",
    "    qt = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "    X_train_qt = qt.fit_transform(X_train_enhanced)\n",
    "    X_test_qt = qt.transform(X_test_enhanced)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_final = scaler.fit_transform(X_train_qt)\n",
    "    X_test_final = scaler.transform(X_test_qt)\n",
    "    \n",
    "    # Train ensemble\n",
    "    print(\"Training advanced ensemble...\")\n",
    "    ensemble = AdvancedEnsemble(X_train_final.shape[1])\n",
    "    ensemble.fit(X_train_final, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    print(\"Making predictions...\")\n",
    "    y_pred = ensemble.predict(X_test_final)\n",
    "    final_r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ULTIMATE NEURAL NETWORK RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"XGBoost baseline:                    0.849273\")\n",
    "    print(f\"Ultimate Neural Network:             {final_r2:.6f}\")\n",
    "    print(f\"Gap closed:                          {abs(0.849273 - final_r2):.6f}\")\n",
    "    \n",
    "    if final_r2 > 0.84:\n",
    "        print(\"🎉 EXCELLENT! Very close to XGBoost performance!\")\n",
    "    elif final_r2 > 0.82:\n",
    "        print(\"✅ GOOD! Getting close to XGBoost performance!\")\n",
    "    else:\n",
    "        print(\"📈 Room for improvement, but still decent!\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return final_r2\n",
    "\n",
    "# Run the ultimate approach\n",
    "ultimate_neural_network_approach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
