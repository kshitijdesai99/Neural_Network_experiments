{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb51f552",
   "metadata": {},
   "source": [
    "# Experiment 4: Can a neural network learn to classifiy arithmetic operations?\n",
    "\n",
    "\n",
    "## Objective\n",
    "The experiment aims to find out if nn can learn to classifiy arithmetic operations.\n",
    "\n",
    "## Problem Specification\n",
    "- **Input**: x, y, z\n",
    "- **Output**: can be +,-,*,/\n",
    "- **Task**: Implementation of a neural network that can learn to classify arithmetic operations\n",
    "\n",
    "## Expected Outcome\n",
    "- A neural network can learn to classify arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e630d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different layer size combinations for arithmetic operations classifier\n",
      "======================================================================\n",
      "\n",
      "Testing with first layer: 8 neurons, second layer: 8 neurons\n",
      "  Epoch 0: Train Loss: 1.6969, Val Accuracy: 0.2130\n",
      "  Epoch 50: Train Loss: 1.3582, Val Accuracy: 0.3100\n",
      "  Epoch 100: Train Loss: 1.2877, Val Accuracy: 0.3940\n",
      "  Epoch 150: Train Loss: 1.1695, Val Accuracy: 0.4700\n",
      "× Early stopping at epoch 151\n",
      "  Best validation accuracy: 0.4880\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Division     |    Addition    \n",
      "   7     |    4     |    3     |    Addition     |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Addition     |    Division    \n",
      "\n",
      "Accuracy on examples: 1/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   34.32%\n",
      " LeakyReLU:   32.30%\n",
      "    Linear:   33.38%\n",
      "\n",
      "Total weight magnitude: 1.044358\n",
      "\n",
      "Testing with first layer: 16 neurons, second layer: 8 neurons\n",
      "  Epoch 0: Train Loss: 1.5132, Val Accuracy: 0.2340\n",
      "  Epoch 50: Train Loss: 1.2325, Val Accuracy: 0.3670\n",
      "  Epoch 100: Train Loss: 1.0798, Val Accuracy: 0.6270\n",
      "  Epoch 150: Train Loss: 0.9200, Val Accuracy: 0.7060\n",
      "  Epoch 200: Train Loss: 0.7614, Val Accuracy: 0.7580\n",
      "  Epoch 250: Train Loss: 0.6479, Val Accuracy: 0.7950\n",
      "  Epoch 300: Train Loss: 0.5651, Val Accuracy: 0.8110\n",
      "  Epoch 350: Train Loss: 0.5008, Val Accuracy: 0.8340\n",
      "  Epoch 400: Train Loss: 0.4502, Val Accuracy: 0.8570\n",
      "  Epoch 450: Train Loss: 0.4111, Val Accuracy: 0.8650\n",
      "× Early stopping at epoch 455\n",
      "  Best validation accuracy: 0.8650\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |   Subtraction   |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |   Subtraction   |    Division    \n",
      "\n",
      "Accuracy on examples: 3/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   36.41%\n",
      " LeakyReLU:   30.71%\n",
      "    Linear:   32.88%\n",
      "\n",
      "Total weight magnitude: 1.698758\n",
      "\n",
      "Testing with first layer: 12 neurons, second layer: 12 neurons\n",
      "  Epoch 0: Train Loss: 1.3928, Val Accuracy: 0.2640\n",
      "  Epoch 50: Train Loss: 1.1455, Val Accuracy: 0.5440\n",
      "  Epoch 100: Train Loss: 0.9674, Val Accuracy: 0.6800\n",
      "  Epoch 150: Train Loss: 0.7602, Val Accuracy: 0.7840\n",
      "  Epoch 200: Train Loss: 0.6071, Val Accuracy: 0.8290\n",
      "  Epoch 250: Train Loss: 0.5175, Val Accuracy: 0.8560\n",
      "  Epoch 300: Train Loss: 0.4588, Val Accuracy: 0.8600\n",
      "× Early stopping at epoch 308\n",
      "  Best validation accuracy: 0.8620\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |   Subtraction   |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Division     |    Division    \n",
      "\n",
      "Accuracy on examples: 4/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   35.01%\n",
      " LeakyReLU:   31.01%\n",
      "    Linear:   33.98%\n",
      "\n",
      "Total weight magnitude: 1.564242\n",
      "\n",
      "Testing with first layer: 16 neurons, second layer: 16 neurons\n",
      "  Epoch 0: Train Loss: 1.5444, Val Accuracy: 0.2020\n",
      "  Epoch 50: Train Loss: 1.1604, Val Accuracy: 0.5550\n",
      "  Epoch 100: Train Loss: 0.9788, Val Accuracy: 0.7180\n",
      "× Early stopping at epoch 145\n",
      "  Best validation accuracy: 0.7570\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |    Division     |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Division     |    Division    \n",
      "\n",
      "Accuracy on examples: 3/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   33.42%\n",
      " LeakyReLU:   32.38%\n",
      "    Linear:   34.21%\n",
      "\n",
      "Total weight magnitude: 1.175720\n",
      "\n",
      "Testing with first layer: 24 neurons, second layer: 12 neurons\n",
      "  Epoch 0: Train Loss: 1.5102, Val Accuracy: 0.2390\n",
      "  Epoch 50: Train Loss: 1.1749, Val Accuracy: 0.4970\n",
      "  Epoch 100: Train Loss: 0.9663, Val Accuracy: 0.7280\n",
      "  Epoch 150: Train Loss: 0.7345, Val Accuracy: 0.8120\n",
      "  Epoch 200: Train Loss: 0.5767, Val Accuracy: 0.8420\n",
      "  Epoch 250: Train Loss: 0.4709, Val Accuracy: 0.8630\n",
      "× Early stopping at epoch 275\n",
      "  Best validation accuracy: 0.8650\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |   Subtraction   |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Division     |    Division    \n",
      "\n",
      "Accuracy on examples: 4/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   34.83%\n",
      " LeakyReLU:   31.51%\n",
      "    Linear:   33.66%\n",
      "\n",
      "Total weight magnitude: 1.414057\n",
      "\n",
      "Testing with first layer: 32 neurons, second layer: 16 neurons\n",
      "  Epoch 0: Train Loss: 1.7475, Val Accuracy: 0.1800\n",
      "  Epoch 50: Train Loss: 1.0895, Val Accuracy: 0.6770\n",
      "  Epoch 100: Train Loss: 0.8647, Val Accuracy: 0.7220\n",
      "  Epoch 150: Train Loss: 0.6845, Val Accuracy: 0.7690\n",
      "  Epoch 200: Train Loss: 0.5564, Val Accuracy: 0.8400\n",
      "  Epoch 250: Train Loss: 0.4726, Val Accuracy: 0.8700\n",
      "× Early stopping at epoch 296\n",
      "  Best validation accuracy: 0.8720\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |   Subtraction   |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Division     |    Division    \n",
      "\n",
      "Accuracy on examples: 4/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   36.78%\n",
      " LeakyReLU:   30.16%\n",
      "    Linear:   33.06%\n",
      "\n",
      "Total weight magnitude: 1.363243\n",
      "\n",
      "Testing with first layer: 16 neurons, second layer: 32 neurons\n",
      "  Epoch 0: Train Loss: 1.4532, Val Accuracy: 0.3510\n",
      "  Epoch 50: Train Loss: 1.0124, Val Accuracy: 0.7130\n",
      "  Epoch 100: Train Loss: 0.7521, Val Accuracy: 0.7670\n",
      "  Epoch 150: Train Loss: 0.5622, Val Accuracy: 0.8310\n",
      "  Epoch 200: Train Loss: 0.4542, Val Accuracy: 0.8490\n",
      "  Epoch 250: Train Loss: 0.3858, Val Accuracy: 0.8670\n",
      "  Epoch 300: Train Loss: 0.3352, Val Accuracy: 0.8830\n",
      "× Early stopping at epoch 341\n",
      "  Best validation accuracy: 0.8910\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |   Subtraction   |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Division     |    Division    \n",
      "\n",
      "Accuracy on examples: 4/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   34.19%\n",
      " LeakyReLU:   32.48%\n",
      "    Linear:   33.33%\n",
      "\n",
      "Total weight magnitude: 1.617362\n",
      "\n",
      "Testing with first layer: 32 neurons, second layer: 32 neurons\n",
      "  Epoch 0: Train Loss: 1.6370, Val Accuracy: 0.2510\n",
      "  Epoch 50: Train Loss: 0.8981, Val Accuracy: 0.6840\n",
      "  Epoch 100: Train Loss: 0.6492, Val Accuracy: 0.7780\n",
      "  Epoch 150: Train Loss: 0.5068, Val Accuracy: 0.8360\n",
      "  Epoch 200: Train Loss: 0.4192, Val Accuracy: 0.8670\n",
      "  Epoch 250: Train Loss: 0.3566, Val Accuracy: 0.8890\n",
      "  Epoch 300: Train Loss: 0.3078, Val Accuracy: 0.8940\n",
      "  Epoch 350: Train Loss: 0.2693, Val Accuracy: 0.9050\n",
      "× Early stopping at epoch 398\n",
      "  Best validation accuracy: 0.9130\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |   Subtraction   |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Division     |    Division    \n",
      "\n",
      "Accuracy on examples: 4/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   36.89%\n",
      " LeakyReLU:   30.20%\n",
      "    Linear:   32.91%\n",
      "\n",
      "Total weight magnitude: 1.557382\n",
      "\n",
      "Testing with first layer: 32 neurons, second layer: 64 neurons\n",
      "  Epoch 0: Train Loss: 1.7545, Val Accuracy: 0.2600\n",
      "  Epoch 50: Train Loss: 0.8202, Val Accuracy: 0.7280\n",
      "  Epoch 100: Train Loss: 0.5662, Val Accuracy: 0.8150\n",
      "  Epoch 150: Train Loss: 0.4409, Val Accuracy: 0.8630\n",
      "  Epoch 200: Train Loss: 0.3650, Val Accuracy: 0.8810\n",
      "  Epoch 250: Train Loss: 0.3124, Val Accuracy: 0.8940\n",
      "  Epoch 300: Train Loss: 0.2692, Val Accuracy: 0.9080\n",
      "× Early stopping at epoch 313\n",
      "  Best validation accuracy: 0.9080\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |   Subtraction   |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Division     |    Division    \n",
      "\n",
      "Accuracy on examples: 4/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   36.28%\n",
      " LeakyReLU:   31.50%\n",
      "    Linear:   32.23%\n",
      "\n",
      "Total weight magnitude: 1.438728\n",
      "\n",
      "Testing with first layer: 48 neurons, second layer: 32 neurons\n",
      "  Epoch 0: Train Loss: 1.5265, Val Accuracy: 0.2140\n",
      "  Epoch 50: Train Loss: 0.8009, Val Accuracy: 0.7640\n",
      "  Epoch 100: Train Loss: 0.5484, Val Accuracy: 0.8470\n",
      "  Epoch 150: Train Loss: 0.4358, Val Accuracy: 0.8640\n",
      "  Epoch 200: Train Loss: 0.3631, Val Accuracy: 0.8830\n",
      "  Epoch 250: Train Loss: 0.3069, Val Accuracy: 0.8990\n",
      "  Epoch 300: Train Loss: 0.2646, Val Accuracy: 0.9110\n",
      "  Epoch 350: Train Loss: 0.2325, Val Accuracy: 0.9180\n",
      "× Early stopping at epoch 381\n",
      "  Best validation accuracy: 0.9220\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |   Subtraction   |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Division     |    Division    \n",
      "\n",
      "Accuracy on examples: 4/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   35.84%\n",
      " LeakyReLU:   32.06%\n",
      "    Linear:   32.11%\n",
      "\n",
      "Total weight magnitude: 1.548067\n",
      "\n",
      "Testing with first layer: 64 neurons, second layer: 32 neurons\n",
      "  Epoch 0: Train Loss: 1.7425, Val Accuracy: 0.2120\n",
      "  Epoch 50: Train Loss: 0.8201, Val Accuracy: 0.7360\n",
      "  Epoch 100: Train Loss: 0.5566, Val Accuracy: 0.8320\n",
      "  Epoch 150: Train Loss: 0.4339, Val Accuracy: 0.8740\n",
      "  Epoch 200: Train Loss: 0.3651, Val Accuracy: 0.8890\n",
      "  Epoch 250: Train Loss: 0.3187, Val Accuracy: 0.8970\n",
      "× Early stopping at epoch 294\n",
      "  Best validation accuracy: 0.9040\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |   Subtraction   |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Division     |    Division    \n",
      "\n",
      "Accuracy on examples: 4/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   35.86%\n",
      " LeakyReLU:   31.46%\n",
      "    Linear:   32.67%\n",
      "\n",
      "Total weight magnitude: 1.379110\n",
      "\n",
      "Testing with first layer: 24 neurons, second layer: 48 neurons\n",
      "  Epoch 0: Train Loss: 1.6765, Val Accuracy: 0.2340\n",
      "  Epoch 50: Train Loss: 0.8986, Val Accuracy: 0.6530\n",
      "  Epoch 100: Train Loss: 0.6452, Val Accuracy: 0.7740\n",
      "  Epoch 150: Train Loss: 0.4957, Val Accuracy: 0.8470\n",
      "  Epoch 200: Train Loss: 0.4117, Val Accuracy: 0.8690\n",
      "  Epoch 250: Train Loss: 0.3554, Val Accuracy: 0.8910\n",
      "  Epoch 300: Train Loss: 0.3156, Val Accuracy: 0.9040\n",
      "  Epoch 350: Train Loss: 0.2867, Val Accuracy: 0.9050\n",
      "× Early stopping at epoch 373\n",
      "  Best validation accuracy: 0.9100\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |   Subtraction   |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Division     |    Division    \n",
      "\n",
      "Accuracy on examples: 4/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   37.84%\n",
      " LeakyReLU:   30.58%\n",
      "    Linear:   31.58%\n",
      "\n",
      "Total weight magnitude: 1.508903\n",
      "\n",
      "Testing with first layer: 20 neurons, second layer: 40 neurons\n",
      "  Epoch 0: Train Loss: 1.4007, Val Accuracy: 0.3940\n",
      "  Epoch 50: Train Loss: 0.9539, Val Accuracy: 0.6780\n",
      "  Epoch 100: Train Loss: 0.6823, Val Accuracy: 0.7870\n",
      "  Epoch 150: Train Loss: 0.5061, Val Accuracy: 0.8440\n",
      "  Epoch 200: Train Loss: 0.4111, Val Accuracy: 0.8730\n",
      "  Epoch 250: Train Loss: 0.3480, Val Accuracy: 0.8890\n",
      "× Early stopping at epoch 298\n",
      "  Best validation accuracy: 0.8940\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |   Subtraction   |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Division     |    Division    \n",
      "\n",
      "Accuracy on examples: 4/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   35.86%\n",
      " LeakyReLU:   31.25%\n",
      "    Linear:   32.90%\n",
      "\n",
      "Total weight magnitude: 1.575033\n",
      "\n",
      "Testing with first layer: 40 neurons, second layer: 20 neurons\n",
      "  Epoch 0: Train Loss: 1.5568, Val Accuracy: 0.3060\n",
      "  Epoch 50: Train Loss: 0.9728, Val Accuracy: 0.6900\n",
      "  Epoch 100: Train Loss: 0.6798, Val Accuracy: 0.7770\n",
      "  Epoch 150: Train Loss: 0.5117, Val Accuracy: 0.8410\n",
      "  Epoch 200: Train Loss: 0.4306, Val Accuracy: 0.8650\n",
      "  Epoch 250: Train Loss: 0.3773, Val Accuracy: 0.8810\n",
      "  Epoch 300: Train Loss: 0.3348, Val Accuracy: 0.8870\n",
      "× Early stopping at epoch 306\n",
      "  Best validation accuracy: 0.8880\n",
      "\n",
      "Testing Model with Examples:\n",
      "------------------------------------------------------------\n",
      "   a     |    b     |    c     |    Predicted    |    Expected    \n",
      "------------------------------------------------------------\n",
      "   2     |    3     |    5     |    Addition     |    Addition    \n",
      "   7     |    4     |    3     |   Subtraction   |   Subtraction  \n",
      "   5     |    6     |    30    | Multiplication  | Multiplication \n",
      "   10    |    2     |    5     |    Division     |    Division    \n",
      "\n",
      "Accuracy on examples: 4/4\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   36.80%\n",
      " LeakyReLU:   30.75%\n",
      "    Linear:   32.44%\n",
      "\n",
      "Total weight magnitude: 1.459580\n",
      "\n",
      "====================================================================================================\n",
      "Configuration Results (sorted by test accuracy):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer 1  | Layer 2  |  Epochs  |  Val Accuracy   |  Test Accuracy  |  Success  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "   48    |    32    |   381    | 0.9220          | 0.9080          | ×\n",
      "   32    |    32    |   398    | 0.9130          | 0.9060          | ×\n",
      "   32    |    64    |   313    | 0.9080          | 0.9000          | ×\n",
      "   24    |    48    |   373    | 0.9100          | 0.9000          | ×\n",
      "   16    |    32    |   341    | 0.8910          | 0.8950          | ×\n",
      "   64    |    32    |   294    | 0.9040          | 0.8920          | ×\n",
      "   20    |    40    |   298    | 0.8940          | 0.8900          | ×\n",
      "   40    |    20    |   306    | 0.8880          | 0.8820          | ×\n",
      "   32    |    16    |   296    | 0.8720          | 0.8610          | ×\n",
      "   16    |    8     |   455    | 0.8650          | 0.8580          | ×\n",
      "   24    |    12    |   275    | 0.8650          | 0.8490          | ×\n",
      "   12    |    12    |   308    | 0.8620          | 0.8470          | ×\n",
      "   16    |    16    |   145    | 0.7570          | 0.7440          | ×\n",
      "   8     |    8     |   151    | 0.4880          | 0.5000          | ×\n",
      "\n",
      "Best Overall Configuration:\n",
      "- Layer 1: 48 neurons\n",
      "- Layer 2: 32 neurons\n",
      "- Trained for 381 epochs\n",
      "- Validation Accuracy: 0.9220\n",
      "- Test Accuracy: 0.9080\n",
      "- Success: No\n",
      "\n",
      "Final activation analysis for the best model:\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   35.84%\n",
      " LeakyReLU:   32.06%\n",
      "    Linear:   32.11%\n",
      "\n",
      "Total weight magnitude: 1.548067\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ArithmeticClassifier(nn.Module):\n",
    "    def __init__(self, first_layer_size, second_layer_size):\n",
    "        super(ArithmeticClassifier, self).__init__()\n",
    "        \n",
    "        # Input layer (3 inputs: a, b, c where c = operation(a, b))\n",
    "        self.input = nn.Linear(3, first_layer_size)\n",
    "        \n",
    "        # Triple activation for first hidden layer\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        self.identity = nn.Identity()\n",
    "        \n",
    "        # Weights for activation functions\n",
    "        self.act_weights = nn.Parameter(torch.ones(3, first_layer_size) / 3)\n",
    "        \n",
    "        # Second hidden layer (standard with ReLU)\n",
    "        self.hidden = nn.Linear(first_layer_size, second_layer_size)\n",
    "        self.hidden_activation = nn.ReLU()\n",
    "        \n",
    "        # Output layer (4 classes: add, subtract, multiply, divide)\n",
    "        self.output = nn.Linear(second_layer_size, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First hidden layer with triple activation\n",
    "        x = self.input(x)\n",
    "        \n",
    "        # Apply weighted activations\n",
    "        activated = (self.tanh(x) * self.act_weights[0].unsqueeze(0) + \n",
    "                    self.leaky_relu(x) * self.act_weights[1].unsqueeze(0) +\n",
    "                    self.identity(x) * self.act_weights[2].unsqueeze(0))\n",
    "        \n",
    "        # Second hidden layer\n",
    "        x = self.hidden(activated)\n",
    "        x = self.hidden_activation(x)\n",
    "        \n",
    "        # Output layer (no softmax here as it's included in CrossEntropyLoss)\n",
    "        return self.output(x)\n",
    "\n",
    "def analyze_activations(model):\n",
    "    # Get weights assigned to each activation function\n",
    "    with torch.no_grad():\n",
    "        weights = model.act_weights.detach().cpu()\n",
    "        \n",
    "        # Calculate importance of each activation\n",
    "        activation_names = ['Tanh', 'LeakyReLU', 'Linear']\n",
    "        importance = torch.mean(torch.abs(weights), dim=1)\n",
    "        \n",
    "        # Normalize to percentage\n",
    "        total = torch.sum(importance)\n",
    "        if total > 0:\n",
    "            importance = 100 * importance / total\n",
    "            \n",
    "            print(\"\\nActivation Function Analysis:\")\n",
    "            print(\"-\" * 40)\n",
    "            for i, name in enumerate(activation_names):\n",
    "                print(f\"{name:>10}: {importance[i].item():>7.2f}%\")\n",
    "                \n",
    "            print(f\"\\nTotal weight magnitude: {total.item():.6f}\")\n",
    "        else:\n",
    "            print(\"\\nActivation Function Analysis:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(\"All activation weights are zero\")\n",
    "\n",
    "def generate_data(num_samples=1000):\n",
    "    \"\"\"Generate training data for arithmetic operations\"\"\"\n",
    "    a = np.random.uniform(-10, 10, num_samples)\n",
    "    b = np.random.uniform(-10, 10, num_samples)\n",
    "    \n",
    "    # Avoid division by zero or very small numbers\n",
    "    b[np.abs(b) < 0.1] = 0.1 * np.sign(b[np.abs(b) < 0.1])\n",
    "    \n",
    "    # Create results for all operations\n",
    "    results = np.zeros((num_samples, 4))\n",
    "    results[:, 0] = a + b      # Addition\n",
    "    results[:, 1] = a - b      # Subtraction\n",
    "    results[:, 2] = a * b      # Multiplication\n",
    "    results[:, 3] = a / b      # Division\n",
    "    \n",
    "    # For each sample, select one operation randomly\n",
    "    selected_ops = np.random.randint(0, 4, num_samples)\n",
    "    c = np.zeros(num_samples)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        c[i] = results[i, selected_ops[i]]\n",
    "    \n",
    "    # Create input features [a, b, c] and labels [operation]\n",
    "    X = np.column_stack((a, b, c))\n",
    "    y = selected_ops\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def train_and_evaluate(first_layer_size, second_layer_size, max_epochs=500):\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create model\n",
    "    model = ArithmeticClassifier(first_layer_size, second_layer_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Generate data\n",
    "    X_train, y_train = generate_data(5000)\n",
    "    X_val, y_val = generate_data(1000)\n",
    "    X_test, y_test = generate_data(1000)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.LongTensor(y_test)\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    best_model_state = None\n",
    "    patience = 20\n",
    "    no_improve = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            val_accuracy = (predicted == y_val_tensor).sum().item() / y_val_tensor.size(0)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            # Print progress occasionally\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"  Epoch {epoch}: Train Loss: {loss.item():.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "            \n",
    "            # Track best model\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "            \n",
    "            # Check if we've reached target accuracy\n",
    "            if val_accuracy >= 0.99:\n",
    "                print(f\"✓ Target achieved with layers ({first_layer_size}, {second_layer_size}) at epoch {epoch+1}\")\n",
    "                print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "                \n",
    "                # Load best model\n",
    "                if best_model_state:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                \n",
    "                # Final test evaluation\n",
    "                test_outputs = model(X_test_tensor)\n",
    "                _, predicted = torch.max(test_outputs, 1)\n",
    "                test_accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "                \n",
    "                return {\n",
    "                    'model': model,\n",
    "                    'layer1': first_layer_size,\n",
    "                    'layer2': second_layer_size,\n",
    "                    'epochs': epoch+1,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_accuracies': val_accuracies,\n",
    "                    'success': True\n",
    "                }\n",
    "            \n",
    "            # Early stopping\n",
    "            if no_improve >= patience:\n",
    "                print(f\"× Early stopping at epoch {epoch+1}\")\n",
    "                print(f\"  Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "                \n",
    "                # Load best model\n",
    "                if best_model_state:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                \n",
    "                # Final test evaluation\n",
    "                test_outputs = model(X_test_tensor)\n",
    "                _, predicted = torch.max(test_outputs, 1)\n",
    "                test_accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "                \n",
    "                return {\n",
    "                    'model': model,\n",
    "                    'layer1': first_layer_size,\n",
    "                    'layer2': second_layer_size,\n",
    "                    'epochs': epoch+1,\n",
    "                    'val_accuracy': best_val_accuracy,\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_accuracies': val_accuracies,\n",
    "                    'success': best_val_accuracy >= 0.99\n",
    "                }\n",
    "    \n",
    "    # Load best model before final evaluation\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Final test evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        test_accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "    \n",
    "    print(f\"× Reached max epochs ({max_epochs})\")\n",
    "    print(f\"  Best validation accuracy: {best_val_accuracy:.4f}, Test accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'layer1': first_layer_size,\n",
    "        'layer2': second_layer_size,\n",
    "        'epochs': max_epochs,\n",
    "        'val_accuracy': best_val_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_losses': train_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'success': best_val_accuracy >= 0.99\n",
    "    }\n",
    "\n",
    "def test_examples(model):\n",
    "    \"\"\"Test model on specific examples with all four operations\"\"\"\n",
    "    operations = ['Addition', 'Subtraction', 'Multiplication', 'Division']\n",
    "    \n",
    "    examples = [\n",
    "        [2, 3, 5],      # 2 + 3 = 5\n",
    "        [7, 4, 3],      # 7 - 4 = 3\n",
    "        [5, 6, 30],     # 5 * 6 = 30\n",
    "        [10, 2, 5],     # 10 / 2 = 5\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting Model with Examples:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'a':^8} | {'b':^8} | {'c':^8} | {'Predicted':^15} | {'Expected':^15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, example in enumerate(examples):\n",
    "            a, b, c = example\n",
    "            input_tensor = torch.FloatTensor([example])\n",
    "            output = model(input_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            pred_op = operations[predicted.item()]\n",
    "            expected_op = operations[i]\n",
    "            \n",
    "            print(f\"{a:^8} | {b:^8} | {c:^8} | {pred_op:^15} | {expected_op:^15}\")\n",
    "            \n",
    "            if pred_op == expected_op:\n",
    "                correct += 1\n",
    "    \n",
    "    print(f\"\\nAccuracy on examples: {correct}/{len(examples)}\")\n",
    "\n",
    "# Test different combinations of layer sizes\n",
    "print(\"Testing different layer size combinations for arithmetic operations classifier\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define combinations to test\n",
    "layer_combinations = [\n",
    "    (8, 8),\n",
    "    (16, 8),\n",
    "    (12, 12),\n",
    "    (16, 16),\n",
    "    (24, 12),\n",
    "    (32, 16),\n",
    "    (16, 32)\n",
    "]\n",
    "\n",
    "additional_combinations = [\n",
    "    (32, 32),    # Balanced larger model\n",
    "    (32, 64),    # Wider second layer\n",
    "    (48, 32),    # Wider first layer\n",
    "    (64, 32),    # Even wider first layer \n",
    "    (24, 48),    # Another asymmetric configuration\n",
    "    (20, 40),    # Smaller but similar ratio to the best so far\n",
    "    (40, 20)     # Reverse ratio experiment\n",
    "]\n",
    "\n",
    "layer_combinations.extend(additional_combinations)\n",
    "\n",
    "results = []\n",
    "\n",
    "for layer1, layer2 in layer_combinations:\n",
    "    print(f\"\\nTesting with first layer: {layer1} neurons, second layer: {layer2} neurons\")\n",
    "    result = train_and_evaluate(layer1, layer2)\n",
    "    \n",
    "    test_examples(result['model'])\n",
    "    analyze_activations(result['model'])\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # If we've found a successful model, we can stop\n",
    "    if result['success'] and result['test_accuracy'] >= 0.99:\n",
    "        print(f\"\\n✓ Found minimum configuration with layers ({layer1}, {layer2})\")\n",
    "        break\n",
    "\n",
    "# Show summary of all configurations\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"Configuration Results (sorted by test accuracy):\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Layer 1':^8} | {'Layer 2':^8} | {'Epochs':^8} | {'Val Accuracy':^15} | {'Test Accuracy':^15} | {'Success':^10}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Sort by test accuracy (best first)\n",
    "for r in sorted(results, key=lambda x: x['test_accuracy'], reverse=True):\n",
    "    print(f\"{r['layer1']:^8} | {r['layer2']:^8} | {r['epochs']:^8} | {r['val_accuracy']:.4f}{' ':^9} | {r['test_accuracy']:.4f}{' ':^9} | {'✓' if r['success'] else '×'}\")\n",
    "\n",
    "# Find the best configuration\n",
    "best_config = max(results, key=lambda x: x['test_accuracy'])\n",
    "\n",
    "print(\"\\nBest Overall Configuration:\")\n",
    "print(f\"- Layer 1: {best_config['layer1']} neurons\")\n",
    "print(f\"- Layer 2: {best_config['layer2']} neurons\")\n",
    "print(f\"- Trained for {best_config['epochs']} epochs\")\n",
    "print(f\"- Validation Accuracy: {best_config['val_accuracy']:.4f}\")\n",
    "print(f\"- Test Accuracy: {best_config['test_accuracy']:.4f}\")\n",
    "print(f\"- Success: {'Yes' if best_config['success'] else 'No'}\")\n",
    "\n",
    "# Final activation analysis\n",
    "print(\"\\nFinal activation analysis for the best model:\")\n",
    "analyze_activations(best_config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3fe214e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the best configuration for visualization\n",
      "======================================================================\n",
      "  Epoch 0: Train Loss: 1.3696, Val Accuracy: 0.3530\n",
      "  Epoch 50: Train Loss: 1.1865, Val Accuracy: 0.5330\n",
      "  Epoch 100: Train Loss: 1.0826, Val Accuracy: 0.6310\n",
      "  Epoch 150: Train Loss: 0.9914, Val Accuracy: 0.6740\n",
      "  Epoch 200: Train Loss: 0.9050, Val Accuracy: 0.7000\n",
      "  Epoch 250: Train Loss: 0.8156, Val Accuracy: 0.7610\n",
      "  Epoch 300: Train Loss: 0.7269, Val Accuracy: 0.8110\n",
      "  Epoch 350: Train Loss: 0.6430, Val Accuracy: 0.8160\n",
      "  Epoch 400: Train Loss: 0.5640, Val Accuracy: 0.8300\n",
      "  Epoch 450: Train Loss: 0.4951, Val Accuracy: 0.8720\n",
      "  Epoch 500: Train Loss: 0.4360, Val Accuracy: 0.8980\n",
      "  Epoch 550: Train Loss: 0.3852, Val Accuracy: 0.9190\n",
      "  Epoch 600: Train Loss: 0.3425, Val Accuracy: 0.9330\n",
      "  Epoch 650: Train Loss: 0.3067, Val Accuracy: 0.9450\n",
      "  Epoch 700: Train Loss: 0.2764, Val Accuracy: 0.9470\n",
      "  Epoch 750: Train Loss: 0.2510, Val Accuracy: 0.9470\n",
      "  Epoch 800: Train Loss: 0.2288, Val Accuracy: 0.9700\n",
      "  Epoch 850: Train Loss: 0.2094, Val Accuracy: 0.9940\n",
      "  Epoch 900: Train Loss: 0.1919, Val Accuracy: 0.9970\n",
      "✓ Target achieved with layers (32, 64, 32) at epoch 944\n",
      "  Validation Accuracy: 1.0000\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   34.13%\n",
      " LeakyReLU:   32.80%\n",
      "    Linear:   33.06%\n",
      "\n",
      "Total weight magnitude: 1.243132\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxQAAAK9CAYAAAC95yoDAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaalJREFUeJzt3Qd4FFXXwPGTQAi9V6X3ForoS7EgRRAQCCAo0kFABFGaGKSjoiAgRUUFAQWkKKKAIlWQ3qQIiApBpEkLIC1Ast9z7vvtvrtJgOyaZHaS/89nnt2dmd29Sa7LnD333BvgcDgcAgAAAAA+CPTlSQAAAACgCCgAAAAA+IyAAgAAAIDPCCgAAAAA+IyAAgAAAIDPCCgAAAAA+IyAAgAAAIDPCCgAAAAA+IyAAgAAAIDPCCgAIA6///671KtXT7JkySIBAQGyePHiBH39o0ePmtedOXNmgr6unT3++ONmAwDYCwEFAL91+PBh6d69uxQtWlTSpk0rmTNnlocfflgmTpwo169fT9T37tChg+zbt0/efPNN+fzzz+XBBx+U5KJjx44mmNHfZ1y/Rw2m9Lhu7777rtevf/LkSRk+fLjs3r07gVoMAPBnqa1uAADEZdmyZdKyZUsJDg6W9u3bS/ny5eXmzZuyYcMGGTBggOzfv18+/vjjRHlvvcjevHmzvP7669KrV69EeY9ChQqZ9wkKChIrpE6dWq5duyZLliyRVq1aeRybM2eOCeBu3Ljh02trQDFixAgpXLiwVKpUKd7PW7FihU/vBwCwFgEFAL8THh4uzz77rLnoXrNmjeTLl891rGfPnvLHH3+YgCOxnD171txmzZo10d5Dv/3Xi3araKCm2Z4vvvgiVkAxd+5cadSokXz11VdJ0hYNbNKnTy9p0qRJkvcDACQshjwB8DtjxoyRK1euyPTp0z2CCafixYvLyy+/7Hp8+/ZtGTVqlBQrVsxcKOs344MGDZLIyEiP5+n+p556ymQ5/vOf/5gLeh1O9dlnn7nO0aE6GsgozYTohb8+zzlUyHnfnT5Hz3O3cuVKeeSRR0xQkjFjRilVqpRp071qKDSAevTRRyVDhgzmuU2bNpWDBw/G+X4aWGmb9Dyt9ejUqZO5OI+v5557Tr7//nu5ePGia9/27dvNkCc9FtOFCxekf//+EhISYn4mHTLVoEED2bNnj+ucH3/8UR566CFzX9vjHDrl/Dm1RkKzTTt37pTHHnvMBBLO30vMGgoddqZ/o5g/f/369SVbtmwmEwIAsB4BBQC/o8Nw9EK/Ro0a8Tr/+eefl6FDh8oDDzwgEyZMkJo1a8ro0aNNliMmvQh/+umn5YknnpBx48aZC1O9KNchVKp58+bmNVTr1q1N/cR7773nVfv1tTRw0YBm5MiR5n2aNGkiGzduvOvzVq1aZS6Wz5w5Y4KGvn37yqZNm0wmQQOQmDSz8M8//5ifVe/rRbsONYov/Vn1Yn/RokUe2YnSpUub32VMR44cMcXp+rONHz/eBFxaZ6K/b+fFfZkyZczPrLp162Z+f7pp8OB0/vx5E4jocCj93daqVSvO9mmtTK5cuUxgERUVZfZ99NFHZmjU5MmT5b777ov3zwoASEQOAPAjly5dcuhHU9OmTeN1/u7du835zz//vMf+/v37m/1r1qxx7StUqJDZt379ete+M2fOOIKDgx39+vVz7QsPDzfnjR071uM1O3ToYF4jpmHDhpnznSZMmGAenz179o7tdr7HjBkzXPsqVarkyJ07t+P8+fOufXv27HEEBgY62rdvH+v9Onfu7PGazZo1c+TIkeOO7+n+c2TIkMHcf/rppx116tQx96Oiohx58+Z1jBgxIs7fwY0bN8w5MX8O/f2NHDnStW/79u2xfjanmjVrmmNTp06N85hu7n744Qdz/htvvOE4cuSII2PGjI7Q0NB7/owAgKRDhgKAX7l8+bK5zZQpU7zO/+6778ytfpvvrl+/fuY2Zq1F2bJlzZAiJ/0GXIcj6bfvCcVZe/HNN99IdHR0vJ5z6tQpMyuSZkuyZ8/u2l+hQgWTTXH+nO5eeOEFj8f6c+m3/87fYXzo0CYdpnT69Gkz3Epv4xrupHQ4WWDgf//Z0IyBvpdzONeuXbvi/Z76OjocKj506l6d6UuzHppR0SFQmqUAAPgPAgoAfkXH5SsdyhMff/75p7nI1boKd3nz5jUX9nrcXcGCBWO9hg57ioiIkITyzDPPmGFKOhQrT548ZujVggUL7hpcONupF+cx6TCic+fOydWrV+/6s+jPobz5WRo2bGiCt/nz55vZnbT+Iebv0knbr8PBSpQoYYKCnDlzmoBs7969cunSpXi/5/333+9VAbZOXatBlgZckyZNkty5c8f7uQCAxEdAAcDvAgodG//LL7949byYRdF3kipVqjj3OxwOn9/DOb7fKV26dLJ+/XpTE9GuXTtzwa1BhmYaYp77b/ybn8VJAwP95n/WrFny9ddf3zE7od566y2TCdJ6iNmzZ8sPP/xgis/LlSsX70yM8/fjjZ9//tnUlSit2QAA+BcCCgB+R4t+dVE7XQviXnRGJr2Y1ZmJ3P39999m9iLnjE0JQTMA7jMiOcXMgijNmtSpU8cULx84cMAskKdDitauXXvHn0MdOnQo1rFff/3VZAN05qfEoEGEXrRrViiuQnanL7/80hRQ6+xbep4OR6pbt26s30l8g7v40KyMDo/SoWpa5K0zgOlMVAAA/0FAAcDvvPrqq+biWYcMaWAQkwYbOgOQc8iOijkTk17IK11PIaHotLQ6tEczDu61D/rNfszpVWNyLvAWcypbJ50eV8/RTIH7BbpmanRWI+fPmRg0SNBpd6dMmWKGit0tIxIz+7Fw4UI5ceKExz5n4BNX8OWtgQMHyrFjx8zvRf+mOm2vzvp0p98jACDpsbAdAL+jF+46fakOE9L6AfeVsnUaVb2I1eJlVbFiRXOBqatm6wWsTmG6bds2cwEaGhp6xylJfaHfyusFbrNmzaR3795mzYcPP/xQSpYs6VGUrAXEOuRJgxnNPOhwnQ8++EDy589v1qa4k7Fjx5rpVKtXry5dunQxK2nr9Ki6xoROI5tYNJsyePDgeGWO9GfTjIFO6avDj7TuQqf4jfn30/qVqVOnmvoMDTCqVq0qRYoU8apdmtHR39uwYcNc09jOmDHDrFUxZMgQk60AAFiPDAUAv6TrNmgmQNeM0NmSdIXs1157zazHoOs6aHGu07Rp08z6CzoU5pVXXjEXomFhYTJv3rwEbVOOHDlMNkIXY9MsigYtugZE48aNY7VdC6Y//fRT0+7333/f1B1ouzQ4uBMdPrR8+XLzPrquhhYjV6tWzaxf4e3FeGLQBeh09iytndCFBTWI0lm0ChQo4HFeUFCQ+d1oRkNnotL1PNatW+fVe+nwq86dO0vlypXl9ddf95jJSt9b+8CWLVsS7GcDAPguQOeO/RfPBwAAAJCCkaEAAAAA4DMCCgAAAAA+I6AAAAAA4DMCCgAAAAA+I6AAAAAA4DMCCgAAAAA+I6AAAAAA4LNkuVJ2usq9rG4CUoiI7VOsbgIAALaU1o+vQq28lrz+s/2uLchQAAAAAPCZH8eGAAAAgAUC+M7dG/y2AAAAAPiMgAIAAACAzxjyBAAAALgLCLC6BbZChgIAAACAz8hQAAAAAO4oyvYKvy0AAAAAPiNDAQAAALijhsIrZCgAAAAA+IyAAgAAAIDPGPIEAAAAuKMo2yv8tgAAAAD4jAwFAAAA4I6ibK+QoQAAAADgMwIKAAAAAD5jyBMAAADgjqJsr/DbAgAAAOAzMhQAAACAO4qyvUKGAgAAAIDPyFAAAAAA7qih8Aq/LQAAAAA+I6AAAAAA4DOGPAEAAADuKMr2ChkKAAAAAD4joAAAAABiFmVbtXlh9OjR8tBDD0mmTJkkd+7cEhoaKocOHfI45/HHH5eAgACP7YUXXvA459ixY9KoUSNJnz69eZ0BAwbI7du3490OhjwBAAAANrRu3Trp2bOnCSo0ABg0aJDUq1dPDhw4IBkyZHCd17VrVxk5cqTrsQYOTlFRUSaYyJs3r2zatElOnTol7du3l6CgIHnrrbfi1Q4CCgAAAMCGli9f7vF45syZJsOwc+dOeeyxxzwCCA0Y4rJixQoTgKxatUry5MkjlSpVklGjRsnAgQNl+PDhkiZNmnu2gyFPAAAAQMyibIu2yMhIuXz5ssem++Lj0qVL5jZ79uwe++fMmSM5c+aU8uXLS1hYmFy7ds11bPPmzRISEmKCCaf69eub992/f3+83peAAgAAAPATo0ePlixZsnhsuu9eoqOj5ZVXXpGHH37YBA5Ozz33nMyePVvWrl1rgonPP/9c2rZt6zp++vRpj2BCOR/rsfhgyBMAAADgJytlh4WFSd++fT32BQcH3/N5Wkvxyy+/yIYNGzz2d+vWzXVfMxH58uWTOnXqyOHDh6VYsWIJ0mYyFAAAAICfCA4OlsyZM3ts9wooevXqJUuXLjVZiPz589/13KpVq5rbP/74w9xqbcXff//tcY7z8Z3qLmIioAAAAABsOG2sw+EwwcTXX38ta9askSJFitzzObt37za3mqlQ1atXl3379smZM2dc56xcudIEMmXLlo1XOxjyBAAAANhQz549Ze7cufLNN9+YtSicNQ9ad5EuXTozrEmPN2zYUHLkyCF79+6VPn36mBmgKlSoYM7VaWY1cGjXrp2MGTPGvMbgwYPNa8dnqJUiQwEAAADY0IcffmhmdtLF6zTj4Nzmz59vjuuUrzodrAYNpUuXln79+kmLFi1kyZIlrtdIlSqVGS6lt5qt0IJtXYfCfd2KeyFDAQAAALgLDBA7cDgcdz1eoEABs/jdvRQqVEi+++47n9tBhgIAAACAz8hQAAAAAH4ybawd8dsCAAAA4DMCCgAAAAA+Y8gTAAAA4C7AHkXZ/oIMBQAAAACfkaEAAAAA3FGU7RV+WwAAAAB8RoYCAAAAcEcNhVfIUAAAAADwGQEFAAAAAJ8x5AkAAABwR1G2V/htAQAAAPAZGQoAAADAHUXZ9gsoLl68KNu2bZMzZ85IdHS0x7H27dtb1i4AAAAAfh5QLFmyRNq0aSNXrlyRzJkzS4BbRKj3CSgAAAAA/2V5DUW/fv2kc+fOJqDQTEVERIRru3DhgtXNAwAAQEosyrZqsyHLW33ixAnp3bu3pE+f3uqmAAAAALBbQFG/fn3ZsWOH1c0AAAAA/kuH4Fu12ZDlNRSNGjWSAQMGyIEDByQkJESCgoI8jjdp0sSytgEAAADw84Cia9eu5nbkyJGxjmlRdlRUlAWtAgAAQIpl01qGFBtQxJwmFgAAAIB9EH4BAAAAsHdAsW7dOmncuLEUL17cbFo38dNPP1ndLAAAAKREFGXbK6CYPXu21K1b10wbq9PH6pYuXTqpU6eOzJ071+rmAQAAALiLAIfD4RALlSlTRrp16yZ9+vTx2D9+/Hj55JNP5ODBg16/ZrrKvRKwhcCdRWyfYnUTAACwpbSWV/LeWbqnrPv3/fpS+13HWp6hOHLkiBnuFJMOewoPD7ekTQAAAABsElAUKFBAVq9eHWv/qlWrzDEAAAAA/svyZFO/fv1M3cTu3bulRo0aZt/GjRtl5syZMnHiRKubBwAAgJSGdSjsFVD06NFD8ubNK+PGjZMFCxa46irmz58vTZs2tbp5AAAAAPw5oFDNmjUzGwAAAGA5m07fahXyOQAAAADslaHInj27/Pbbb5IzZ07Jli2bBNwlCrxw4UKStg0AAACAnwcUEyZMkEyZMrnu3y2gAAAAAJIURdn+H1B06NDBdb9jx45WNCFF6N+5noTWriglC+eR65G3ZOueI/L6xG/k9z/PeJxXtUIRGd7zKXkopLBERUXL3t9OSOMX35cbkbfM8YXvdZeKJe+XXNkzScTla7J26yEZPOkbOXX2kkU/Gexq3tw5MmvGdDl37qyULFVaXhs0REIqVLC6WUiG6GtIKvQ1wA9qKFKlSiVnznhe4Krz58+bY/Ddow8Ul6nz10vN9u/KUz2mSOrUqWTph70kfdo0HsHEN1NelNVbfpVH246VR9qOlanz1kl09P8WUF+//TdpO/BTqdhspDw3YJoULZBT5o7tYtFPBbta/v138u6Y0dL9xZ4yb+HXUqpUaenRvYv5fx1ISPQ1JBX6WjKmo2es2mwowOFw/O/K0QKBgYFy+vRpyZ07t8f+kydPSrFixeT69etev2a6yvZbsjwp5MyWUf5a87bU7TJBNu46bPatm9VPVm/9VUZ+sCzer9OoZogsGN9VslR9RW7fjpaULGL7FKubYBttnm0p5cqHyKDBQ83j6OhoqVenprR+rp106drN6uYhGaGvIanQ1/6dtH4x12jc0oV+bNl7X19sv75j2Z9y0qRJ5lbrJ6ZNmyYZM2Z0HYuKipL169dL6dKlrWpespQ5Y1pzG3HpmrnNlS2j/KdCEZn3/Q5ZO7OvFMmfU347+rcMn7JENu0+EudrZMucXp5t8KBs2ROe4oMJxN+tmzfl4IH90qVrd48vE6pVqyF79/xsaduQvNDXkFToa8kcNRT2CCi0GFtpgmTq1Kkew5vSpEkjhQsXNvuRMDRwG9v/adn082E5cPiU2acBhHq9e0MJm/C17D10XNo89R/57qOXpErLt+TwsbOu57/Ru6m88OxjkiFdsGzdGy7Ne/O3QfxFXIwwXxTkyJHDY78+Dg+PO3gFfEFfQ1KhrwF+EFCEh4eb21q1asmiRYvM9LG+iIyMNJs7R3SUBARSf+HuvbBWUq54PqnT6b+BnAoM/O84velfbZDPv91i7u85dFwe/08p6dC0ugyd/K3r3AmfrZKZizdLwXzZ5fXuDWTaqHYEFQAAALC+KHvt2rU+BxNq9OjRkiVLFo/t9t87E7SNdjdhYEtp+Gh5qd91kpw4c9G1/9TZy+b24JHTHucfCj8tBfJ6/k3OX7wqfxw7I2u2/irtX5shDR4tbwq6gfjIljWbyULGLFTUx7oeDZBQ6GtIKvS1ZI6ibP/PUPTt21dGjRolGTJkMPfvZvz48Xc9HhYWFus1cj86MEHamVyCiSa1K0q9rhPlz5OeH3r6+OSZi1KysGdBfPFCuWXFxgN3fE1nZiNNkB9XU8GvBKVJI2XKlpOtWzZL7Tp1XcWLW7dulmdbt7W6eUhG6GtIKvQ14H8suSL8+eef5datW677dxKfBe+Cg4PN5vE8hju5hjk90+BBadnnY7ly9YbkyfHfxQQvXbnhWmNiwqxVMviFRrLvtxNmuFPbxlWlVOE88tyA6eb4Q+ULSZVyhUztxcV/rkmR/Llk2IuNTH2F1lIA8dWuQycZMmiglCtXXsqHVJDZn88ys7iFNmtuddOQzNDXkFToa8kXiy7bIKDQYU5x3UfC6t7qMXO7ctorHvu7Dv1cZi/Zau5PmfujpA0OkjH9Wki2LOlNYKFrVoQfP2eOX7txS5rWrmiCjgzp0sjpc5dkxaaD8s4nn8rNW7ct+KlgV082aCgRFy7IB1MmmQWgSpUuIx98NE1yMDQACYy+hqRCXwP8ZB2KxMA6FEgqrEMBAEDyW4cifYtPLXvva191Frux5E/ZvHn8U4E6AxQAAACQVBjyZINZntxnZMqcObOsXr1aduzY4Tq+c+dOs0+PAwAAAPBflmQoZsyY4bo/cOBAadWqlcfidrpQzIsvvmiCDQAAACBJkaCw1zoUn376qfTv399jpWy9r1PB6jEAAAAA/svygOL27dvy66+/xtqv+3Q+ZwAAACCpayis2uzI8vr6Tp06SZcuXeTw4cPyn//8x+zbunWrvP322+YYAAAAAP9leUDx7rvvSt68eWXcuHFy6tQpsy9fvnwyYMAA6devn9XNAwAAAODPAUVgYKC8+uqrZrt8+bLZRzE2AAAArGLXoUcpNqBwRyABAAAA2IslAUXlypXjHfnt2rUr0dsDAAAAOJGhsEFAERoa6rp/48YN+eCDD6Rs2bJSvXp1s2/Lli2yf/9+sxYFAAAAAP9lSUAxbNgw1/3nn39eevfuLaNGjYp1zl9//WVB6wAAAADYpoZi4cKFsmPHjlj727ZtKw8++CCL2wEAACBJMeTJZgvbpUuXTjZu3Bhrv+5LmzatJW0CAAAAYJMMxSuvvCI9evQwxdfuC9tNnz5dhg4danXzAAAAkNKQoLBXQPHaa69J0aJFZeLEiTJ79myzTwu0Z82aJWXKlLG6eQAAAAD8OaBQrVq1MpvSxe2++OILGTt2rOzcuVOioqKsbh4AAABSEGoobFZD4bR+/Xrp0KGD3HfffTJu3DipXbu2mT4WAAAAgP+yNENx+vRpmTlzpqmX0MyEZikiIyNl8eLFZtgTAAAAAP9mWYaicePGUqpUKdm7d6+89957cvLkSZk8ebJVzQEAAABcQ56s2uzIsgzF999/bxa00xmeSpQoYVUzAAAAANgxQ7Fhwwb5559/pEqVKlK1alWZMmWKnDt3zqrmAAAAAAYZCpsEFNWqVZNPPvlETp06Jd27d5d58+aZguzo6GhZuXKlCTYAAAAA+DfLZ3nKkCGDdO7c2WQs9u3bJ/369ZO3335bcufOLU2aNLG6eQAAAAD8OaBwp0XaY8aMkePHj5u1KAAAAICkxpAnGwcUTqlSpZLQ0FD59ttvrW4KAAAAAH9fKRsAAADwG/ZMFFjGLzMUAAAAAOyBDAUAAADgxq61DFYhQwEAAADAZwQUAAAAAHzGkCcAAADADUOevEOGAgAAAIDPyFAAAAAAbshQeIcMBQAAAACfEVAAAAAA8BlDngAAAAB3jHjyChkKAAAAAD4jQwEAAAC4oSjbO2QoAAAAAPiMDAUAAADghgyFd8hQAAAAAPAZAQUAAAAAnzHkCQAAAHDDkCfvkKEAAAAA4DMyFAAAAIAbMhTeIUMBAAAAwGcEFAAAAAB8xpAnAAAAwB0jnrxChgIAAACAz8hQAAAAAG4oyvYOGQoAAAAAPiNDAQAAALghQ+EdMhQAAAAAfEZAAQAAAMBnDHkCAAAA3DDkyTtkKAAAAAD4jAwFAAAA4I4EhVfIUAAAAADwGQEFAAAAAJ8x5AkAAABwQ1G2d8hQAAAAADY0evRoeeihhyRTpkySO3duCQ0NlUOHDnmcc+PGDenZs6fkyJFDMmbMKC1atJC///7b45xjx45Jo0aNJH369OZ1BgwYILdv3453OwgoAAAAgBgZCqs2b6xbt84EC1u2bJGVK1fKrVu3pF69enL16lXXOX369JElS5bIwoULzfknT56U5s2bu45HRUWZYOLmzZuyadMmmTVrlsycOVOGDh0a73YEOBwOhyQz6Sr3sroJSCEitk+xugkAANhSWj8eeF+o9xLL3vvPSY19fu7Zs2dNhkEDh8cee0wuXbokuXLlkrlz58rTTz9tzvn111+lTJkysnnzZqlWrZp8//338tRTT5lAI0+ePOacqVOnysCBA83rpUmT5p7vS4YCAAAA8BORkZFy+fJlj033xYcGECp79uzmdufOnSZrUbduXdc5pUuXloIFC5qAQultSEiIK5hQ9evXN++7f//+eL0vAQUAAADgJ0OeRo8eLVmyZPHYdN+9REdHyyuvvCIPP/ywlC9f3uw7ffq0yTBkzZrV41wNHvSY8xz3YMJ53HksPvw42QQAAACkLGFhYdK3b1+PfcHBwfd8ntZS/PLLL7JhwwZJagQUAAAAgJ9MGxscHByvAMJdr169ZOnSpbJ+/XrJnz+/a3/evHlNsfXFixc9shQ6y5Mec56zbds2j9dzzgLlPOdeGPIEAAAA2JDD4TDBxNdffy1r1qyRIkWKeByvUqWKBAUFyerVq137dFpZnSa2evXq5rHe7tu3T86cOeM6R2eMypw5s5QtWzZe7SBDAQAAALizybp2PXv2NDM4ffPNN2YtCmfNg9ZdpEuXztx26dLFDKHSQm0NEl566SUTROgMT0qnmdXAoV27djJmzBjzGoMHDzavHd9MCQEFAAAAYEMffvihuX388cc99s+YMUM6duxo7k+YMEECAwPNgnY6W5TO4PTBBx+4zk2VKpUZLtWjRw8TaGTIkEE6dOggI0eOjHc7WIcC+BdYhwIAgOS3DkWRPssse+/wCY3Ebvz4T+k7LvKQVLJV62N1E5BCRGyZYHUTACDFsLIo244oygYAAADgs2SZoQAAAAB8RYbCO2QoAAAAAPiMgAIAAACAzxjyBAAAALhhxJN3yFAAAAAA8BkZCgAAAMANRdneIUMBAAAAwGdkKAAAAAA3JCi8Q4YCAAAAgM8IKAAAAAD4jCFPAAAAgBuKsr1DhgIAAACAz8hQAAAAAG5IUHiHDAUAAAAAnxFQAAAAAPAZQ54AAAAAN4GBjHnyBhkKAAAAAD4jQwEAAAC4oSjbO2QoAAAAAPiMDAUAAADghoXtvEOGAgAAAIDPCCgAAAAA+IwhTwAAAIAbRjx5hwwFAAAAAJ+RoQAAAADcUJTtHTIUAAAAAHxGQAEAAADAZwx5AgAAANww5Mk7ZCgAAAAA+IwMBQAAAOCGBIV3yFAAAAAA8BkZCgAAAMANNRTeIUMBAAAAwGcEFAAAAAB8xpAnAAAAwA0jnrxDhgIAAACAz8hQAAAAAG4oyvYOGQoAAAAA9s5QXLx4UbZt2yZnzpyR6Ohoj2Pt27e3rF0AAAAA/DygWLJkibRp00auXLkimTNn9kgx6X0CCgAAACQlRjzZbMhTv379pHPnziag0ExFRESEa7tw4YLVzQMAAADgzxmKEydOSO/evSV9+vRWNwUAAACgKNtuGYr69evLjh07rG4GAAAAADtmKBo1aiQDBgyQAwcOSEhIiAQFBXkcb9KkiWVtAwAAQMpDgsJmAUXXrl3N7ciRI+NMN0VFRVnQKgAAAAC2CChiThMLAAAAwD4sDygAAAAAf0JRts2KstW6deukcePGUrx4cbNp3cRPP/1kdbMAAAAA+HtAMXv2bKlbt66ZNlanj9UtXbp0UqdOHZk7d67VzQMAAEAKowkKqzY7snzI05tvviljxoyRPn36uPZpUDF+/HgZNWqUPPfcc5a2DwAAAIAfZyiOHDlihjvFpMOewsPDLWkTAAAAAJsEFAUKFJDVq1fH2r9q1SpzDAAAAEjqomyrNjuyfMhTv379zBCn3bt3S40aNcy+jRs3ysyZM2XixIlWNw8AAACAPwcUPXr0kLx588q4ceNkwYIFZl+ZMmVk/vz50rRpU6ubBwAAgBTGpomClBtQqGbNmpkNAAAAgL34RUABAAAA+Au71jKkqIAie/bs8ttvv0nOnDklW7Zsd/2jXbhwIUnbBgAAAMDPA4oJEyZIpkyZXPeJAgEAAAB7siSg6NChg+t+x44drWgCAAAAECe+67bZOhSpUqWSM2fOxNp//vx5cwwAAACA/7K8KNvhcMS5PzIyUtKkSZPk7QEAAEDKxnB8mwQUkyZNcv3Bpk2bJhkzZnQdi4qKkvXr10vp0qWtah4AAAAAfw4otBjbmaGYOnWqx/AmzUwULlzY7AcAAADgvywLKMLDw81trVq1ZNGiRWb6WAAAAMBqDHmyWQ3F2rVrrW4CAAAAALvO8tSiRQt55513Yu0fM2aMtGzZ0pI2AQAAIOXSBIVVmx1ZHlBo8XXDhg1j7W/QoIE5BgAAAMB/WR5QXLlyJc7pYYOCguTy5cuWtAkAAACATQKKkJAQmT9/fqz98+bNk7Jly1rSJgAAAKTsomyrNjuyvCh7yJAh0rx5czl8+LDUrl3b7Fu9erV88cUXsnDhQqublyLMmztHZs2YLufOnZWSpUrLa4OGSEiFClY3CzbSv2MdCa1VQUoWzi3XI2/J1r1H5fXJS+T3P8+6zvnho57yWJXiHs/75KtN0nv0//4/f/yhEjLshQZSrng+uXr9psxZtl2GffCdREVFJ+nPA/vjcw1Jhb4G+EGGonHjxrJ48WL5448/5MUXX5R+/frJ8ePHZdWqVRIaGmp185K95d9/J++OGS3dX+wp8xZ+LaVKlZYe3bvI+fPnrW4abOTRB4rJ1IUbpGanifJUz6mSOnUqWTrlBUmf1nM44/RFm6Vw/aGu7fVJ37qOhZS4TxZP7CYrNv8q1dqMk3aDPpNGj5WXN3o9ZcFPBDvjcw1Jhb6WfFGUbbOAQjVq1Eg2btwoV69elXPnzsmaNWukZs2aVjcrRfh81gxp/nQrCW3WQooVLy6Dh42QtGnTyuJFX1ndNNhI094fy+yl2+XgkdOy7/eT0m34XCmYL7tULpPf47zrN27K3+f/cW3/XI10HXv6iUryy+8nZfS0FXLk+DnZsOuwvD5piXRv+bBkTB9swU8Fu+JzDUmFvgb4UUABa9y6eVMOHtgv1arXcO0LDAyUatVqyN49P1vaNthb5ozpzG3E5Wse+59pUEX+WjVKdsx/VUb2bCTpgoNcx4LTpJYbN297nK/Dp9KlTRMrMAHuhM81JBX6WvJGDYXNaiiioqJkwoQJsmDBAjl27JjcvHnT4/iFCxcsa1tyF3Exwvz+c+TI4bFfH4eHH7GsXbA3/TAc2y9UNu0+IgcOn3btn798lxw7dUFOnb0sISXyyRsvNZaShXLLs6/OMMdXbj4kvVrXlFb1K8uXK3dL3hyZZdDz9cyxfDkzW/bzwF74XENSoa8BfpShGDFihIwfP16eeeYZuXTpkvTt29cUaWuUP3z48Hs+PzIy0kwv677pPgDWeG9gCylXLJ+0H/SZx/5Pv94sq7Yckv2HT8m85buky7A50rR2BSly/3//MV699ZAMmvStTAprKZc2jZW9i8Lkh40HzbHoaIclPwsAALBBQDFnzhz55JNPTDF26tSppXXr1jJt2jQZOnSobNmy5Z7PHz16tGTJksVjG/vO6CRpu91ly5pNUqVKFat4TB/nzJnTsnbBvia82lwaPlJW6r/wvpw4c+mu527/5Zi5LVbgf31t0px1kvfxQVLyqZGSv+4QWbJun9kffoICR8QPn2tIKvS15I2ibJsFFKdPnzZrUaiMGTOaLIV66qmnZNmyZfd8flhYmHmO+zZgYFiitzs5CEqTRsqULSdbt2x27YuOjpatWzdLhYqVLW0b7BlMNHk8RJ7s8YH8efLeQxUrlrrf3J4+F3sBy1PnLsuNyFvSqv4D8tfpCPn51+OJ0mYkP3yuIanQ1wA/qqHInz+/nDp1SgoWLCjFihWTFStWyAMPPCDbt2+X4OB7z+yi58Q874ZnXSfuol2HTjJk0EApV668lA+pILM/nyXXr1+X0GbNrW4abDbM6Zknq0jLftPlyrVIyZMjk9l/6coNExjosKZnnnzADGE6f+mqmSJ2TN9Q+WnnH/LLH6dcr9OnXS1ZselXiXY4pGmtELO+RdvXZjHkCV7hcw1Jhb6WfAXaNVWQUgOKZs2amYXsqlatKi+99JK0bdtWpk+fbgq0+/TpY3Xzkr0nGzSUiAsX5IMpk8yiPKVKl5EPPpomOUjXwgvdWz5ibld+3Mtjf9fhc810srduR0nt/5Q0RdcZ0qWR439flMVr9srb01d4nF+vRhl5tfMTEhyUykw/qwGKBhiAN/hcQ1KhrwH/FeBwOPzqqz+tm9i0aZOUKFHCLHrnCzIUSCrZqhH0ImlEbJlgdRMAIEGltfxr7Tt7Ysq963gTy8pe1cRuLP1T3rp1S7p37y5DhgyRIkWKmH3VqlUzGwAAAGAFRjzZqCg7KChIvvqK1SQBAAAAu7J8lqfQ0FBZvHix1c0AAAAADFbK9o7lo9e0VmLkyJGyceNGqVKlimTIkMHjeO/evS1rGwAAAAA/Dyh0RqesWbPKzp07zeZOozQCCgAAACSlQHsmClJuQBEeHm51EwAAAADYtYZChztdu3Yt1n5dGEaPAQAAAPBflgcUI0aMkCtXrsTar0GGHgMAAACSEkXZNgsodF29uH55e/bskezZs1vSJgAAAAB+XkORLVs2VyRWsmRJj6AiKirKZC1eeOEFq5oHAACAFMqmiYKUF1C89957JjvRuXNnM7QpS5YsrmNp0qSRwoULS/Xq1a1qHgAAAAB/Dig6dOhgbosUKSI1atQwq2YDAAAAsBfLp42tWbOmGeL05ZdfysGDB82+smXLStOmTSV1asubBwAAgBQmQBjz5A3Lr9j3798vTZo0kdOnT0upUqXMvnfeeUdy5colS5YskfLly1vdRAAAAAD+OsvT888/L+XKlZPjx4/Lrl27zPbXX39JhQoVpFu3blY3DwAAAClwpWyrNjuyPEOxe/du2bFjh5n1yUnvv/nmm/LQQw9Z2jYAAAAAfp6h0Clj//7771j7z5w5I8WLF7ekTQAAAEi5WNjOBgHF5cuXXdvo0aOld+/epihbhz3ppvdfeeUVU0sBAAAAwH9ZMuQpa9asHhGYrkfRqlUr1z59rBo3bmxmgAIAAADgnywJKNauXWvF2wIAAAD3ZNORRykroNC1JwAAAADYn+WzPK1fv/6uxx977LEkawsAAAAQSIrCXgHF448/Hmufe30FNRQAAACA/7J82tiIiAiPTaeLXb58uVmDYsWKFVY3DwAAAIA/ZyiyZMkSa98TTzwhadKkkb59+8rOnTstaRcAAABSJkY82SxDcSd58uSRQ4cOWd0MAAAAwC+tX7/eLLNw3333mZKBxYsXexzv2LFjrIXznnzySY9zLly4IG3atJHMmTObpR26dOkiV65csVeGYu/evR6PdQ2KU6dOydtvvy2VKlWyrF0AAABImeyyYvXVq1elYsWK0rlzZ2nevHmc52gAMWPGDNfj4OBgj+MaTOi198qVK+XWrVvSqVMn6datm8ydO9c+AYUGDfpHcy5m51StWjX59NNPLWsXAAAAkNQiIyPN5k6DgJiBgGrQoIHZ7kaflzdv3jiPHTx40NQub9++XR588EGzb/LkydKwYUN59913TebDFkOewsPD5ciRI+ZWtz///FOuXbsmmzZtktKlS1vdPAAAAKQwmqCwahs9erSpMXbfdJ+vfvzxR8mdO7eUKlVKevToIefPn3cd27x5sxnm5AwmVN26dSUwMFC2bt0a7/ewLKDQH2Dp0qVSqFAh17Zu3Tqz7kTBggVNqiVmdAYAAAAkZ2FhYXLp0iWPTff5Qoc7ffbZZ7J69Wp55513zLW2ZjScyzKcPn3aBBvuUqdOLdmzZzfH/H7I08iRI80aFE899ZR5vG/fPlMEosUjZcqUkbFjx5o0y/Dhw61qIgAAAJCkgu8wvMkXzz77rOt+SEiIVKhQQYoVK2ayFnXq1JGEYlmGYvfu3R4/yLx586Rq1aryySefmOliJ02aJAsWLLCqeQAAAEjBK2VbtSWmokWLSs6cOeWPP/4wj7W2QteAc3f79m0z89Od6i78KqDQRex0algnZwrGSRe2++uvvyxqHQAAAJC8HD9+3NRQ5MuXzzyuXr26XLx40WPdtzVr1kh0dLT5ot/vAwoNJrQIW928eVN27dplZnZy+ueffyQoKMiq5gEAACCFCrBw84auF6GjfnRTem2t948dO2aODRgwQLZs2SJHjx41dRRNmzaV4sWLS/369c35WmagdRZdu3aVbdu2ycaNG6VXr15mqFR8Z3iyNKDQ6ahee+01+emnn0yhSfr06eXRRx/1WJ9Cx3gBAAAAiG3Hjh1SuXJlsyktG9D7Q4cOlVSpUpnr6SZNmkjJkiVNrXKVKlXMtbd7jcacOXPMzKpaiqDX54888oh8/PHH4g3LirJHjRplFuCoWbOmZMyYUWbNmiVp0qRxHdc1KOrVq2dV8wAAAAC/9vjjj8day83dDz/8cM/X0BmdvFnEzq8CCi0I0eXCdSosDSg0inK3cOFCsx8AAABISnZZKdtfWL5Sti7WcadoCQAAAEAyCCh0/FV86fy2AAAAgF0FkqBI+ICiUqVKJvVzpzFazmN661x5DwAAAEDyF6+Awjm9KwAAAJDcUUORCAFFoUKFvHxZAAAAACmBT+tQfP755/Lwww+bBS/+/PNPs++9996Tb775JqHbBwAAACA5BRQffvihWTRDF77QpbqdNRNZs2Y1QQUAAABgZzriyaotRQQUkydPlk8++URef/11j7UjHnzwQdm3b19Ctw8AAABAclqHQgu0nct7u9MlvK9evZpQ7QIAAAAsQVF2ImcoihQpIrt37461f/ny5VKmTBlvXw4AAABASspQaP1Ez5495caNG2btiW3btskXX3who0ePlmnTpiVOKwEAAAAkj4Di+eefl3Tp0sngwYPl2rVr8txzz5nZniZOnCjPPvts4rQSAAAASCKslJ3IAYVq06aN2TSguHLliuTOnduXlwEAAACQEgMKdebMGTl06JCrcCVXrlwJ2S4AAADAEhRlJ3JR9j///CPt2rUzw5xq1qxpNr3ftm1buXTpkrcvBwAAACAlBRRaQ7F161ZZtmyZWdhOt6VLl8qOHTuke/fuidNKAAAAIIkEWLiliCFPGjz88MMP8sgjj7j21a9f3yx29+STTyZ0+wAAAAAkpwxFjhw5JEuWLLH2675s2bIlVLsAAAAAJMeAQqeL1bUoTp8+7dqn9wcMGCBDhgxJ6PYBAAAASSowIMCyLdkOeapcubJHtfvvv/8uBQsWNJs6duyYBAcHy9mzZ6mjAAAAAFKQeAUUoaGhid8SAAAAwA/YNFHg3wHFsGHDEr8lAAAAAJJ/DQUAAAAA+DxtbFRUlEyYMEEWLFhgaidu3rzpcfzChQveviQAAADgN1gpO5EzFCNGjJDx48fLM888Y1bG1hmfmjdvLoGBgTJ8+HBvXw4AAABASgoo5syZYxax69evn6ROnVpat24t06ZNk6FDh8qWLVsSp5UAAABAEtEEhVVbiggodM2JkJAQcz9jxowmS6GeeuopWbZsWcK3EAAAAEDyCSjy588vp06dMveLFSsmK1asMPe3b99u1qIAAAAAkHJ4XZTdrFkzWb16tVStWlVeeukladu2rUyfPt0UaPfp0ydxWgkAAAAkEbuuWG2bgOLtt9923dfC7EKFCsmmTZukRIkS0rhx44RuHwAAAIDkvA5FtWrVzExPmrF46623EqZVAAAAgEUoyrZoYTutqxgyZEhCvRwAAACA5DjkCQAAAEjOWNjOogwFAAAAgJSHgAIAAABA4g950sLruzl79qzvrQBsKmLLBKubgBQiW8OxVjcBKUTEdwOsbgJgOb5xT6SA4ueff77nOY899piXbw8AAAAgRQQUa9euTdyWAAAAAH6AomzvkNEBAAAA4DMCCgAAAAA+Yx0KAAAAwE0gI568QoYCAAAAgM/IUAAAAABuyFAkQYbip59+krZt20r16tXlxIkTZt/nn38uGzZs8OXlAAAAAKSUgOKrr76S+vXrS7p06czaFJGRkWb/pUuX5K233kqMNgIAAABJOm2sVVuKCCjeeOMNmTp1qnzyyScSFBTk2v/www/Lrl27Erp9AAAAAJJTQHHo0KE4V8TOkiWLXLx4MaHaBQAAACA5BhR58+aVP/74I9Z+rZ8oWrRoQrULAAAAsKwo26otRQQUXbt2lZdfflm2bt1qxnmdPHlS5syZI/3795cePXokTisBAAAAJI9pY1977TWJjo6WOnXqyLVr18zwp+DgYBNQvPTSS4nTSgAAACCJ2LQ22j4BhWYlXn/9dRkwYIAZ+nTlyhUpW7asZMyYMXFaCAAAACD5LWyXJk0aE0gAAAAASLm8Dihq1ap11zly16xZ82/bBAAAAFgmkDFPiRtQVKpUyePxrVu3ZPfu3fLLL79Ihw4dvH05AAAAACkpoJgwYUKc+4cPH27qKQAAAAA783oa1BQuwX5fbdu2lU8//TShXg4AAABAci7Kjmnz5s2SNm3ahHo5AAAAwBKUUCRyQNG8eXOPxw6HQ06dOiU7duyQIUOGePtyAAAAAFJSQJElSxaPx4GBgVKqVCkZOXKk1KtXLyHbBgAAACA5BRRRUVHSqVMnCQkJkWzZsiVeqwAAAACLMG1sIhZlp0qVymQhLl686OXbAAAAAEiOvJ7lqXz58nLkyJHEaQ0AAABgMU1QWLWliIDijTfekP79+8vSpUtNMfbly5c9NgAAAAApR7xrKLToul+/ftKwYUPzuEmTJhLgFkbpbE/6WOssAAAAAKQM8Q4oRowYIS+88IKsXbs2cVsEAAAAWCjQpkOP/D6g0AyEqlmzZmK2BwAAAEBynTbWfYgTAAAAkBwxbWwiBhQlS5a8Z1Bx4cIFL5sAAAAAIEUEFFpHEXOlbAAAACA5IUGRiAHFs88+K7lz55bEoIvlbdu2Tc6cOSPR0dEex9q3b58o7wkAAAAgiQKKxKyfWLJkibRp00auXLkimTNn9ngvvU9AAQAAANh8YTvnLE+JQde36Ny5swkoNFMRERHh2qjJAAAAQFJPG2vVlqwzFDGHISWkEydOSO/evSV9+vSJ9h4AAAAALMxQJKb69evLjh07rG4GAAAAIAEW/pfsi7ITS6NGjWTAgAFy4MABCQkJkaCgII/jTZo0saxtAAAAAPw8oOjatau5HTlyZKxjWpQdFRVlQasAAAAA2CKgSMz6DAAAAMAbdi2OTtE1FAAAAADsyW8CinXr1knjxo2lePHiZtO6iZ9++snqZgEAACCFYdpYGwYUs2fPlrp165ppY3X6WN3SpUsnderUkblz51rdPAAAAAD+XEPx5ptvypgxY6RPnz6ufRpUjB8/XkaNGiXPPfecpe0DAABAyqGTAsFmGYojR46Y4U4x6bCn8PBwS9oEAAAAwCYBRYECBWT16tWx9q9atcocAwAAAOCf/GLIU79+/cwQp927d0uNGjXMvo0bN8rMmTNl4sSJVjcPAAAAKYhdi6NTdEDRo0cPyZs3r4wbN04WLFhg9pUpU0bmz58vTZs2tbp5AAAAAPw5oFDNmjUzGwAAAGAlarJtWEMBAAAAwJ4sy1Bkz55dfvvtN8mZM6dky5btrtNzXbhwIUnbBgAAAMDPA4oJEyZIpkyZXPeZ7xcAAAD+IJDrUnsEFB06dHDd79ixo1XNAAAAAGD3GopUqVLJmTNnYu0/f/68OQYAAAAk5bSxVm125BcBhcPhiHN/ZGSkpEmTJsnbAwAAAMAG08ZOmjTJ3Gr9xLRp0yRjxoyuY1FRUbJ+/XopXbq0hS0EAABASkMJhY0CCi3GdmYopk6d6jG8STMThQsXNvsBAAAA+CdLA4rw8HBzW6tWLVm0aJGZPhYAAACAffjFStlr1661ugkAAACAESiMebJdQKGOHz8u3377rRw7dkxu3rzpcWz8+PGWtQsAAACAnwcUq1evliZNmkjRokXl119/lfLly8vRo0dNbcUDDzxgdfMAAACQglCUbcNpY8PCwqR///6yb98+SZs2rXz11Vfy119/Sc2aNaVly5ZWNw8AAACAPwcUBw8elPbt25v7qVOnluvXr5spZEeOHCnvvPOO1c0DAAAA4M8BRYYMGVx1E/ny5ZPDhw+7jp07d87ClgEAACClYaVsG9ZQVKtWTTZs2CBlypSRhg0bSr9+/czwJ51KVo8BAAAA8E9+EVDoLE5Xrlwx90eMGGHuz58/X0qUKMEMTwAAAEhSgVRl2y+g0Nmd3Ic/sTo2AAAAYA9+UUOxfft22bp1a6z9um/Hjh2WtAkAAACATQKKnj17mmliYzpx4oQ5BgAAACQVHfFk1eaN9evXS+PGjeW+++6TgIAAWbx4scdxXdNt6NChZtKjdOnSSd26deX333/3OOfChQvSpk0byZw5s2TNmlW6dOniKkWwVUBx4MCBOBewq1y5sjmGxDVv7hxp8ERteahyiLR5tqXs27vX6iYhmaKv4d/q/2xV2TC5rZxZ/LL8ueBFWTA8VErkz3bH8xe/2UKurxggjWsUd+0LKZpLZoU9Jb/P6S4XlrwiP0/rLD1DWUQVvuFzDVa6evWqVKxYUd5///04j48ZM0YmTZpkygl05I+WFtSvX19u3LjhOkeDif3798vKlStl6dKlJkjp1q2b/QKK4OBg+fvvv2PtP3XqlFmXAoln+fffybtjRkv3F3vKvIVfS6lSpaVH9y5y/vx5q5uGZIa+hoTwaEgBmfrtz1Lz5dny1GsLJXWqQFk6uqWkTxsU69yXmlcRhyP2a1QukUfOXrwmnd5eJg90nSHvfLFFRnZ+TF5oUjlpfggkG3yuJe+ibKs2bzRo0EDeeOMNadasWaxjmp147733ZPDgwdK0aVOpUKGCfPbZZ3Ly5ElXJkPXglu+fLlMmzZNqlatKo888ohMnjxZ5s2bZ86zVUBRr149s1r2pUuXXPsuXrwogwYNkieeeMLStiV3n8+aIc2fbiWhzVpIseLFZfCwEWa18sWLvrK6aUhm6GtICE1f/1Jmr9wvB/88L/uOnJVu734vBfNkMUGCuwpFc8vLLR6SF8Ytj/Uan/3wi/T/cI1s2Hdcjp6+JPNWH5DPVvwiTR8pkYQ/CZIDPteQGCIjI+Xy5csem+7zVnh4uJw+fdoMc3LKkiWLCRw2b95sHuutDnN68MEHXefo+YGBgXHWN/t1QPHuu++aGopChQpJrVq1zFakSBHzSxg3bpzVzUu2bt28KQcP7Jdq1Wu49mkHqlathuzd87OlbUPyQl9DYsmcIdjcRvzzv/R9uuDUMjOskbwyZZX8HXE1Xq+TJUOwx2sA98LnWvJmZQ3F6NGjzYW/+6b7vKXX0SpPHs8vXPSx85je5s6d2+O4jg7Knj2765z48IvxRPfff7/s3btX5syZI3v27DFFI506dZLWrVtLUFDsNDYSRsTFCImKipIcOXJ47NfH4eFHLGsXkh/6GhKD/sM79oXasumX43Lg6DnX/jEv1JYtB07K0s1/xOt1qpW9T56uWUqaDVmUiK1FcsPnGhJLWFiY9O3bN1Z5gD/zi4BCaZGItwUgSlNAMdNAjlTBfv+LBwD8O+/1ekLKFc4pdfrOde1rVK2YPF6poFTrMSter1G2cE5ZMLyZvDl7k6zeeTQRWwsA8aPXsAlxHZs3b15zq3XKOsuTkz6uVKmS65wzZ854PO/27dtm5ifn8/06oPj2229NIYlmIPT+3TRp0uSOxzQFpKtru3t9yDAZPHR4grU1ucqWNZukSpUqVvGYPs6ZM6dl7ULyQ19DQpvQs440rFZU6vabJyfO/W96Qw0miubLKqe/7u1x/hdDmsrGX45L/QHzXftKF8wh373TSj79bo+8M3dLkrYf9sfnWvLmFzUB/5KWD2hQsHr1alcAofUYWhvRo0cP87h69eqmbnnnzp1SpUoVs2/NmjUSHR1tai38PqAIDQ11jdvS+3eic+pqStGbtJBmKHBvQWnSSJmy5WTrls1Su85/C3a0A23dulmebd3W6uYhGaGvIaGDiSYPl5B6/efJn6f/N5mHenf+NpmxfJ/Hvp0fd5JXP1ory7Ycdu0rUyiHfD/mGZmzcr8Mn7khydqO5IPPNfiDK1euyB9//OFRiL17925TA1GwYEF55ZVXzCxQJUqUMAHGkCFDzJoVzmvvMmXKyJNPPildu3Y1U8veunVLevXqJc8++6w5z+8DCv2fLq77CZEWunH7XzUtRWnXoZMMGTRQypUrL+VDKsjsz2fJ9evXJbRZc6ubhmSGvoaE8N5LdeWZWmWk5bCv5cr1W5InWwaz/9LVSLlx87Ypwo6rEPuvM5ddwYcOc/p+TCtZteOoTPpqh+s1oqKj5dyl60n8E8HO+FxLvvQLbTvYsWOHmczIyfkle4cOHWTmzJny6quvmrUqtKxAMxE6LaxOE6uzkTlpDbMGEXXq1DETC7Ro0cKsXWHLGgpY48kGDSXiwgX5YMokOXfurJQqXUY++Gia5CBdiwRGX0NC6N74v2tFrBzX2mN/17Hfmelk46PZoyUld9YM8lzdcmZz0oCjdPuPE7jFSM74XIPVHn/8cbPexN0Co5EjR5rtTjSbMXfu/2rRfBHguFsrEpE3kU/v3p5jYe+FDAWA5CZbw7FWNwEpRMR3A6xuAlKItH78tfasHX9Z9t4dHiwgdmPZn3LChAnxOk8jK28DCgAAAMBX9hjw5D8sCyi0aAQAAACAvfldssk5AssuxTAAAABIXgK5DrXnNLvTp0+X8uXLm6pz3fT+tGnTrG4WAAAAAH/PUAwdOlTGjx8vL730kllgQ23evFn69Okjx44du2tlOgAAAJCQyE/YMKD48MMP5ZNPPpHWrVt7rI5doUIFE2QQUAAAAAD+yS+GPOmqfA8++GCs/boE+O3bzAELAAAA+Cu/CCjatWtnshQxffzxx9KmTRtL2gQAAICUSWuyrdrsyC+GPDmLslesWCHVqlUzj7du3WrqJ9q3b+9aRlxprQUAAAAA/+AXAcUvv/wiDzzwgLl/+PBhc5szZ06z6TEnppIFAABAYuOa04YBxdq1a61uAgAAAAC71lAAAAAAsCfLMhTNmzeXmTNnSubMmc39u1m0aFGStQsAAAApG9+42ySgyJIli2t8mgYVjFUDAAAA7MeygGLGjBmu+5qpAAAAAPwBX3TbMKNTu3ZtuXjxYqz9ly9fNscAAAAA+Ce/mOXpxx9/lJs3b8baf+PGDfnpp58saRMAAABSJvITNgoo9u7d67p/4MABOX36tOtxVFSULF++XO6//36LWgcAAADArwOKSpUqmTFqusU1tCldunQyefJkS9oGAAAAwM8DivDwcHE4HFK0aFHZtm2b5MqVy3UsTZo0kjt3bkmVKpWVTQQAAEAKQ1G2jQKKQoUKmdvo6GgrmwEAAADAbgHFt99+G+9zmzRpkqhtAQAAAPxqGlQbsSygCA0NjXfKSQu0AQAAAPgfywIKhjkBAAAA9ucX61AAAAAA/oKibBsGFCNHjrzr8aFDhyZZWwAAAADYLKD4+uuvPR7funXLTCmbOnVqKVasGAEFAAAAkgz5CRsGFD///HOsfZcvX5aOHTtKs2bNLGkTAAAAABvPipU5c2YZMWKEDBkyxOqmAAAAIAXREgqrNjvy24BCXbp0yWwAAAAA/JNfDHmaNGmSx2OHwyGnTp2Szz//XBo0aGBZuwAAAADYIKCYMGGCx+PAwEDJlSuXdOjQQcLCwixrFwAAAFKeQMqy7RdQ6IxOAAAAAOzH0oCic+fO8Trv008/TfS2AAAAAMquxdEpMqCYOXOmFCpUSCpXrmzqJgAAAADYi6UBRY8ePeSLL74wQ546deokbdu2lezZs1vZJAAAAAB2mTb2/fffN7M5vfrqq7JkyRIpUKCAtGrVSn744QcyFgAAALBEgIX/2ZHl61AEBwdL69atZeXKlXLgwAEpV66cvPjii1K4cGG5cuWK1c0DAAAA4O+zPLlPFxsQEGCyE1FRUVY3BwAAACkQRdk2y1BERkaaOoonnnhCSpYsKfv27ZMpU6bIsWPHJGPGjFY3DwAAAIC/Zih0aNO8efNM7YROIauBRc6cOa1sEgAAAFI4FrazUUAxdepUKViwoBQtWlTWrVtntrgsWrQoydsGAAAAwM8Divbt25uaCQAAAAD2ZPnCdgAAAIA/4ftumxVlAwAAALAvv5o2FgAAALAaGQrvkKEAAAAA4DMCCgAAAAA+Y8gTAAAA4CaAdSi8QoYCAAAAgM/IUAAAAABuAklQeIUMBQAAAACfkaEAAAAA3FBD4R0yFAAAAAB8RkABAAAAwGcMeQIAAADcsFK2d8hQAAAAAPAZGQoAAADADUXZ3iFDAQAAAMBnBBQAAAAAfMaQJwAAAMANK2V7hwwFAAAAAJ+RoQAAAADcUJTtHTIUAAAAAHxGQAEAAADAZwx5AgAAANywUrZ3yFAAAAAA8BkZCgAAAMANCQrvkKEAAAAA4DMyFAAAAICbQIoovEKGAgAAAIDPCCgAAAAA+IwhTwBgAxHfDbC6CUghsoVOsboJSCGuL+0l/ooBT94hQwEAAADAZ2QoAAAAAHekKLxChgIAAACAzwgoAAAAAPiMIU8AAACAmwDGPHmFDAUAAAAAn5GhAAAAANywULZ3yFAAAAAA8BkZCgAAAMANCQrvkKEAAAAA4DMCCgAAAAA+Y8gTAAAA4I4xT14hQwEAAADAZ2QoAAAAADcsbOcdMhQAAAAAfEZAAQAAAMBnDHkCAAAA3LBStnfIUAAAAADwGRkKAAAAwA0JCu+QoQAAAADgMzIUAAAAgDtSFF4hQwEAAADAZwQUAAAAAHzGkCcAAADADStle4cMBQAAAACfkaEAAAAA3LCwnXfIUAAAAADwGQEFAAAAAJ8x5AkAAABww4gn75ChAAAAAOAzMhQAAACAO1IUXiFDAQAAAMBnBBQAAABAjIXtrPrPG8OHD5eAgACPrXTp0q7jN27ckJ49e0qOHDkkY8aM0qJFC/n7778loRFQAAAAADZVrlw5OXXqlGvbsGGD61ifPn1kyZIlsnDhQlm3bp2cPHlSmjdvnuBtoIYCAAAAsKnUqVNL3rx5Y+2/dOmSTJ8+XebOnSu1a9c2+2bMmCFlypSRLVu2SLVq1RKsDWQoAAAAgBgrZVu1RUZGyuXLlz023Xcnv//+u9x3331StGhRadOmjRw7dszs37lzp9y6dUvq1q3rOleHQxUsWFA2b96coL8vAgoAAADAT4wePVqyZMnisem+uFStWlVmzpwpy5cvlw8//FDCw8Pl0UcflX/++UdOnz4tadKkkaxZs3o8J0+ePOZYQmLIEwAAAOAns8aGhYVJ3759PfYFBwfHeW6DBg1c9ytUqGACjEKFCsmCBQskXbp0klTIUAAAAAB+Ijg4WDJnzuyx3SmgiEmzESVLlpQ//vjD1FXcvHlTLl686HGOzvIUV83Fv0FAAQAAACQDV65ckcOHD0u+fPmkSpUqEhQUJKtXr3YdP3TokKmxqF69eoK+L0OeAAAAABuulN2/f39p3LixGeakU8IOGzZMUqVKJa1btza1F126dDHDp7Jnz24yHS+99JIJJhJyhidFQAEAAADY0PHjx03wcP78ecmVK5c88sgjZkpYva8mTJgggYGBZkE7nSmqfv368sEHHyR4OwIcDodDkpkbt61uAQAA9pQtdIrVTUAKcX1pL/FX+09ctey9y92fQeyGGgoAAAAAPmPIEwAAAOBGF5hD/JGhAAAAAOAzAgoAAAAAPmPIEwAAAOCGEU/eIUMBAAAAwGdkKAAAAAB3pCi8QoYCAAAAgM8IKAAAAAD4jCFPAAAAgJsAxjzZL6C4evWqvP3227J69Wo5c+aMREdHexw/cuSIZW0DAAAA4OcBxfPPPy/r1q2Tdu3aSb58+SSA5QkBAABgES5FbRhQfP/997Js2TJ5+OGHrW4KAAAAALsFFNmyZZPs2bNb3QwAAACACgo7zvI0atQoGTp0qFy7ds3qpgAAAACwW4Zi3LhxcvjwYcmTJ48ULlxYgoKCPI7v2rXLsrYBAAAA8POAIjQ01OomAAAAAP/FmCf7BRTDhg2zugkAAAAA7BpQOO3cuVMOHjxo7pcrV04qV65sdZMAAACQwrCwnQ0DCl3M7tlnn5Uff/xRsmbNavZdvHhRatWqJfPmzZNcuXJZ3UQAAAAA/jrL00svvST//POP7N+/Xy5cuGC2X375RS5fviy9e/e2unkAAAAA/DlDsXz5clm1apWUKVPGta9s2bLy/vvvS7169SxtGwAAAFIWVsq2YYYiOjo61lSxSvfpMQAAAAD+yS8Citq1a8vLL78sJ0+edO07ceKE9OnTR+rUqWNp2wAAAJCyBFi42ZFfBBRTpkwx9RK6qF2xYsXMVqRIEbNv8uTJVjcPAAAAgD/XUBQoUMCshq11FL/++qvZp/UUdevWtbppAAAAAPw9oFABAQHyxBNPmA0AAACwjF3HHqW0gGLSpEnSrVs3SZs2rbl/N0wdm7jmzZ0js2ZMl3PnzkrJUqXltUFDJKRCBaubhWSIvoakQl/Dv9W/ZRUJrV5USubPJtdv3patB0/L6zM3ye8nLrrOmdzzcaldqYDky55Brty4JVsOnpLBMzfJb8f/d45qW6e09A6tJCXuzyqXr92URRv+kD5T11vwUwGJI8DhcDjEAlojsWPHDsmRI4e5f7fMxZEjR7x67Ru3E6CBKcTy77+TwWGvyuBhIyQkpKLM+XyWrFixXL5Zutz8bYCEQl9DUqGv/TvZQqdY3QS/8M2IxrJw/e+y8/czkjpVgIxoX13KFcoulXvMlWuR/73Q6Fy/nBw6HiF/nf1HsmdKK68/9x+pWCSnlH7+M4mO/u/llQYSLzerJIM+3STbDp2WDGmDpFDuTLJs21FJ6a4v7SX+6sjZG5a9d9FcacVuLAsoEhMBRfy1eballCsfIoMGDzWPdZreenVqSuvn2kmXrt2sbh6SEfoakgp97d8hoIhbzsxp5a+5z0vdgYtk4/7/zUrprnzhHLJ9Smsp+/xnEn76smTNECyHZ3WUFqOWyY97jid5m/0dAUXyCSj8YpanmKKiomT37t0SERFhdVOStVs3b8rBA/ulWvUarn2BgYFSrVoN2bvnZ0vbhuSFvoakQl9DYsmcIdjcRlyJ+0IzfXBqaV+3jISfviTHz10x++pULiCBgQFyX44M8vOHz8kfMzvK7IH1JX/OjEnadvi2sJ1Vmx35RUDxyiuvyPTp013BxGOPPSYPPPCAmf3pxx9/tLp5yVbExQjz+445BEAfnzt3zrJ2IfmhryGp0NeQGPQib2zXR2XT/pNy4M8LHse6NSwvZxd2k/NfvSD1qhSSRoO/kVu3/7sob5G8mSUwIEBebfmgDPhkgzw3+nvJlimtLB3VVIJS+8UlGJAg/KI3f/nll1KxYkVzf8mSJXL06FEzfawubPf666/f9bmRkZFmvQr3TfcBAAAkhPd61DT1E+3H/BDr2Lwff5NqL883Q6F+P3lRZr/2pAQHpXLVgaYJSiX9Pl4vq3Ydk22H/pYOY36Q4vdlkZoV8lvwkwDJOKDQb43y5s1r7n/33XfSsmVLKVmypHTu3Fn27dt31+eOHj1asmTJ4rGNfWd0ErXc3rJlzSapUqWS8+fPe+zXxzlz5rSsXUh+6GtIKvQ1JLQJLzwmDR8qLPUHfS0nzl+NdVxnbTp88pKpq9AMRKn82aRp9aLm2OmIa+b212P/y2qcu3zDbAVyMezJn7FStg0Dijx58siBAwdMmnr58uWutSiuXbtm/mG4m7CwMLl06ZLHNmBgWBK13N6C0qSRMmXLydYtm137tHhx69bNUqFiZUvbhuSFvoakQl9DQgcTTaoXlSdfXyx//v3PPc93XhBqVkJtPnDK3JbIn811TraMwabA+9iZe78eYBd+sbBdp06dpFWrVpIvXz6THnSukL1161YpXbr0XZ8bHBxsNnfM8hR/7Tp0kiGDBkq5cuWlfEgFmf35LLl+/bqENmtuddOQzNDXkFToa0ioYU7P1CwpLd9YJleu3ZI8WdOb/ZeuRcqNm1FSOE9mefqxErJ61zE5d/m63J8jo/RrWUWu34ySH3b8ac794+RFWbL5iLzb7VHpNXmtXL5+U0Z2qG6mml2394TFPyHuyq6pgpQcUAwfPlzKly8vf/31lxnu5AwQNDvx2muvWd28ZO3JBg0l4sIF+WDKJLMAVKnSZeSDj6ZJDoYGIIHR15BU6GtICN0bhZjblW97BqJdJ6yS2at/lchbUfJwuXzSq0lFk3U4c/GabNh/UmoN+FLOXrruOr/L+JUypuujsmj4UxIdLbLhlxPSdNgSuR3138JtIDlgHQoAAODCOhRIKv68DsXR89atQ1E4h/3WobAsQzFp0iTp1q2bpE2b1ty/m969eydZuwAAAJCyBTDmyR4ZiiJFisiOHTvM3OB6/060puLIkSNevTYZCgAAfEOGAknFnzMUf563bgmCQjk8a4PtwLIMRXh4eJz3AQAAACvZdcXqFD1t7IYNG6xuAgAAAAC7BhS1a9c2w54GDRok+/fvt7o5AAAASMFY2M6GAcXJkyelX79+sm7dOgkJCZFKlSrJ2LFj5fjx41Y3DQAAAIC/BxQ5c+aUXr16ycaNG+Xw4cNmLYpZs2ZJ4cKFTfYCAAAAgH/yi4Xt3OnQJ13MrmLFijJkyBCTtQAAAACSCkXZNsxQOGmG4sUXX5R8+fLJc889Z1bPXrZsmdXNAgAAAODPGYqwsDCZN2+eqaV44oknZOLEidK0aVNJnz691U0DAABAikOKwnYBxfr162XAgAHSqlUrU08BAAAAwB5S+8tQJwAAAAD2Y1lA8e2330qDBg0kKCjI3L+bJk2aJFm7AAAAkLJRlG2TgCI0NFROnz4tuXPnNvfvJCAgQKKiopK0bQAAAAD8PKCIjo6O8z4AAABgJRIUNquh0GBi5syZsmjRIjl69KjJSBQtWlRatGgh7dq1M48BAAAA+CdL16FwOBymPuL555+XEydOSEhIiJQrV84EFh07dpRmzZpZ2TwAAACkQPp9tlWbHVmaodDMhE4Zu3r1aqlVq5bHsTVr1pjais8++0zat29vWRsBAAAA+GmG4osvvpBBgwbFCiZU7dq15bXXXpM5c+ZY0jYAAAAAfh5Q7N27V5588sk7HtdpZffs2ZOkbQIAAEDKFmDhf3ZkaUBx4cIFyZMnzx2P67GIiIgkbRMAAAAAm9RQ6PoSqVPfuQmpUqWS27dvJ2mbAAAAkMLZM1GQMgMKneVJZ3MKDg6O83hkZGSStwkAAACATQKKDh063PMcZngCAAAA/JelAcWMGTOsfHsAAAAgFkY82agoGwAAAIC9WZqhAAAAAPyNXVestgoZCgAAAAA+I0MBAAAAuLHrAnNWIUMBAAAAwGcEFAAAAAB8xpAnAAAAwB0jnrxChgIAAACAz8hQAAAAAG5IUHiHDAUAAAAAnxFQAAAAAPAZQ54AAAAAN6yU7R0yFAAAAAB8RoYCAAAAcMNK2d4hQwEAAADAZ2QoAAAAADfUUHiHDAUAAAAAnxFQAAAAAPAZAQUAAAAAnxFQAAAAAPAZRdkAAACAG4qyvUOGAgAAAIDPCCgAAAAA+IwhTwAAAIAbVsr2DhkKAAAAAD4jQwEAAAC4oSjbO2QoAAAAAPiMDAUAAADghgSFd8hQAAAAAPAZAQUAAAAAnzHkCQAAAHDHmCevkKEAAAAA4DMyFAAAAIAbFrbzDhkKAAAAAD4joAAAAADgM4Y8AQAAAG5YKds7ZCgAAAAA+IwMBQAAAOCGBIV3yFAAAAAA8BkBBQAAAACfMeQJAAAAcMeYJ6+QoQAAAADgMzIUAAAAgBtWyvYOGQoAAADApt5//30pXLiwpE2bVqpWrSrbtm1L8jYQUAAAAAAxFrazavPG/PnzpW/fvjJs2DDZtWuXVKxYUerXry9nzpyRpERAAQAAANjQ+PHjpWvXrtKpUycpW7asTJ06VdKnTy+ffvppkraDgAIAAADwE5GRkXL58mWPTffFdPPmTdm5c6fUrVvXtS8wMNA83rx5c5K2OVkWZadNlj9V4tKOOnr0aAkLC5Pg4GCrm4NkjL6GpEJf8831pb2sboLt0NeSHyuvJYe/MVpGjBjhsU+HNA0fPtxj37lz5yQqKkry5MnjsV8f//rrr5KUAhwOhyNJ3xF+SaPfLFmyyKVLlyRz5sxWNwfJGH0NSYW+hqRCX0NCB6gxMxIaqMYMVk+ePCn333+/bNq0SapXr+7a/+qrr8q6detk69atklT4Lh8AAADwE8FxBA9xyZkzp6RKlUr+/vtvj/36OG/evJKUqKEAAAAAbCZNmjRSpUoVWb16tWtfdHS0eeyesUgKZCgAAAAAG+rbt6906NBBHnzwQfnPf/4j7733nly9etXM+pSUCChgaGpNC34oJkNio68hqdDXkFToa7DKM888I2fPnpWhQ4fK6dOnpVKlSrJ8+fJYhdqJjaJsAAAAAD6jhgIAAACAzwgoAAAAAPiMgAIAAACAzwgokhldRVELcu6mY8eOEhoa6nr8+OOPyyuvvHLX58ycOVOyZs2aYO2E//vxxx8lICBALl68KP6MvmlP2rcWL17s1WdVfBQuXNjMcuLN+/xb9MHkwdu+ErOvJdS5gB0RUNjA5s2bzcIljRo1SpTXX7RokYwaNequH3w6i8Bvv/2WKO+PxKGzPvTo0UMKFixoZh7RRW7q168vGzdutO2FFH3TOnpxrxdcL7zwQqxjPXv2NMf0HF8cPXrUPH/37t0e+ydOnGj62L9x6tQpadCggSQU+qB9+65uQUFBZvabJ554Qj799FMzZ7+vfWX79u3SrVu3BD8XsCMCChuYPn26vPTSS7J+/XqzzHpCy549u2TKlOmu56RLl05y586d4O+NxNOiRQv5+eefZdasWeZi59tvvzXZqPPnzyd5W27evJlor03fTDoFChSQefPmyfXr1137bty4IXPnzjWBa0LLkiXLvw5YNZBO7Kk86YP+78knnzQBgwav33//vdSqVUtefvlleeqpp+T27ds+9ZVcuXJJ+vTpE/xcwI4IKPzclStXZP78+eabZs1QxPy27u233zbftmhA0KVLF/OPu7uoqCiz6In+o5wjRw559dVXJeZMwe5DnvT+n3/+KX369HF9o3Onb6I//PBDKVasmFmpsVSpUvL55597HNfnTps2TZo1a2Y+SEuUKGEuapH4dJjSTz/9JO+88475h7NQoUJmwZuwsDBp0qRJnN8I63N0nw51cqcZjQoVKkjatGmlWrVq8ssvv5j9ep4unHPp0iVXX9Ehd85vcTXr1b59e8mcObPrm7mBAwdKyZIlTX8oWrSoDBkyRG7duuXxfkuWLJGHHnrIvF/OnDlN/1H0Tes98MADJqjQrKaT3tdgonLlynf9Fl+HYjr7R0xFihQxt/oa+rfRv/Wdhmf26tXLbBpsaP/QPnS32c9jDmM5fvy4tG7d2nyRkiFDBrMY1NatW82xw4cPS9OmTc1nasaMGU0/XLVqlcf70wftyZmlvf/++00/HjRokHzzzTcmuHD+u+reV2rUqGE+r2JmfTXDoV/uxezn2ge1fzszwvfdd5/07t37jv9PHDt2zPQ17Wf6GdmqVSv5+++/Yw1f1n6jz9X+/uyzz8o///yTyL8pwDcEFH5uwYIFUrp0afMPUtu2bU2K1vmPpx7TD5233npLduzYIfny5ZMPPvjA4/njxo0zH5b6vA0bNsiFCxfk66+/vuP76cVB/vz5ZeTIkebbHN3ioq+h3+7069fPXGB2797dXFyuXbvW47wRI0aYD8q9e/dKw4YNpU2bNqYNSFz6j5Ru+o9jZGTkv3qtAQMGmH6kKXv9lq1x48YmCNB/cPUfSP3H0NlX+vfv73reu+++KxUrVjRZEr3oUxr4an88cOCAGc7yySefyIQJE1zPWbZsmbnA0r6iz1u9erUJhBR90z907txZZsyY4Xqsny3/dkXWbdu2mVu9eNe/q3vAEpNm3FKnTm2eo31o/Pjx5sI8vl/Q1KxZU06cOGEu3vfs2WO+ZHEOe9Hj2he032n/02+1tb/rxZ+iDyYvtWvXNp9RcfU3/VtoNs49WNUv9zRQePTRR2Od/9VXX5nPso8++kh+//1389kbEhIS5/tqf9NgQv/W69atk5UrV8qRI0fM0Dl3GuDq6yxdutRseq5+iQj4JV3YDv6rRo0ajvfee8/cv3XrliNnzpyOtWvXmsfVq1d3vPjiix7nV61a1VGxYkXX43z58jnGjBnjeqyvkT9/fkfTpk1d+2rWrOl4+eWXXY8LFSrkmDBhgsfrzpgxw5ElSxaPdnXt2tXjnJYtWzoaNmzoeqzda/Dgwa7HV65cMfu+//57H38b8MaXX37pyJYtmyNt2rTm7xUWFubYs2ePORYeHm7+Fj///LPr/IiICLPP2b/0Vh/PmzfPdc758+cd6dKlc8yfPz/OfuHeh0JDQ+/ZxrFjxzqqVKnieqx9uk2bNnc8n75pnQ4dOpjPjTNnzjiCg4MdR48eNZv2r7Nnz5pjes6d/k76uTRs2DDXY/19f/3113fsj+7v6f5ZVaZMGUd0dLRr38CBA80+p5jv7f4+H330kSNTpkymH8dXuXLlHJMnT77j6yv6oH+L2Y/cPfPMM67+495XtJ+nTp3asX79eo/PJ+1vcfWFcePGOUqWLOm4efNmnO/jfu6KFSscqVKlchw7dsx1fP/+/eb9t23bZh7r/yvp06d3XL582XXOgAEDzL/xgD8iQ+HHDh06ZL6F0/S80m/l9BsMralQBw8elKpVq3o8p3r16q77OhRFv0FzP0dfQ1P8/5a+98MPP+yxTx/rfnc6VMZJhxfot9lnzpz51++P+NVQaM2NfhOr37TqECVN9Xtb5Orep3SYiGbLYv6d4xJXP9Nv+LSf6NADzaAMHjzY9e2v0iFYderUkX+Dvpm4NEvlHH6pmQq9r0OPkooOu3MONXL2T/1GWId33ov2Lx1Wpf04Lpqh0CxbmTJlzBAm7aPab9z7aHzQB+1D4wj3/uTez+vVqydz5swxj8PDw80EKZq5iEvLli1NbZEO5ezatavJUjlrM2LSfqBDB3VzKlu2rOlz7n1Ehzq51zfqKAT6B/wVAYUf08BBP5A0xaqBgG46LldTqxos2IGON3WnH9zus2ogcWkdgs5mokOONm3aZMakDxs2TAID//u/vns6P2Ytw7+lF0junP8Y69AOTd/rkJLXX3/do2Bbi1uTCn3z3w170oBChx/p/Zi0f8Wsa0jo/uWLe/UvDSb0QlCHkWoNkgYgOmwlsSYVoA9aTy/gnTU8Menn1Zdffmn6rk48oH3hTsOYNDjQLwF12LH2sxdffFEee+yxf9Xv6R+wEwIKP6WBxGeffWbGrus/as5Nx/xqgPHFF1+Yb9GcxYROW7Zscd3XIi79RsP9HH3dnTt33vW9tYjwXt/26XvHnH5UH+u3LPBf+ve5evWq+fZNuY8BjzllZ1x9KiIiwswYpX//+PYVJw1otDhcgwjNXmgRqha4xvzGVsev3wl90z9oxksvsvViSacijkn7l3vfunz5svmG925/VxWfvhTXZ572JZ1a+160f2k/v1OdgvYTDbq1jkcvHDWTphMYxGwrfTB5WLNmjezbt89kc+OidQ460cny5ctNQHGn7ISTBhJaczNp0iSTEdYvUfT14+off/31l9mctK5MJ8agj8CuUlvdAMRNv8HVizeduUkDA3f64afZC/02Tf/x04szTadranb//v0m5eqkhYFaxKX/4GpxtxYw3muhMk2z6iwWOqOEzlYR13AGLdTVYkIdPlC3bl0zM48WtrnPiALr6NSwmoLXb4/1IkrT5lq4P2bMGPOPpP7Dp0NHtG/ot3OaRtfhR3HRAlSdIUxnvtFgQPuDc+Yd7Ss6TESDAC1u1Nlq7jQ1ovZBHTqihY46e44WYMecIECzJzrkSWfH0f6nAfB3333nmm2Fvukf9OLdOTQjrgt5LXbVDIZeXOkwjqFDh971gl+nXNU+qRduWvSsmbWYn3tO2od05jotdN61a5dMnjzZfPESHzp8VLMP2n9Hjx5tvnDRTJl+SaNDp7SPal/Rduu3wZrZi/mNMH3QnnRyitOnT5tgUGdT0r6mfUCnjdXZ6O6UZdW+ov1A+7tz+HFctL/ra+sQY/0MnD17tunT+iVKTNonNGDVAEUnttDPOc1o6IQBCTEkGbACGQo/pQGDfujE9Y+qBhR6cajfcugHnc5SUqVKFfNtr04v605nGWnXrp106NDB/IOpF5bOaTjvRC8g9Vs5vahzfpMdk37I6gwrOpNPuXLlzMwWOp7aOd0jrKVjv/UfNp11RNPu5cuXN31Fx/ZOmTLFNTuP/kOmfUenDX7jjTfifC0NOjQw1fP0H2S9OHJ+o6wzPelCZ1rbo31FA5Y70elqdbpNnfJTp0PUjIVz9icn7T8LFy40dR96jl6YOmcAUvRN/6Hj/XWLi05PrBdHerGmNRb6N9G/2Z3ocE79Vlf/Vnpxr0HvnejFn45V19m/dEE97ZvxXTBM++2KFStMAKND7/SiTvu3M9jRL1yyZctm+rUGFZp90bojd/RBe9IAQgNIDQg1w6Yzbmmf06lj7xbs6kW/jgzQmZ3uttaKBs46a51+uadf4mjwqJ+V+mVMTBqs6vtqX9PPZ/23Xr8I1BozwK4CtDLb6kYAAHAvekGugWbMNS4AANYiQwEAAADAZwQUAAAAAHzGkCcAAAAAPiNDAQAAAMBnBBQAAAAAfEZAAQAAAMBnBBQAAAAAfEZAAQAAAMBnBBQA8C917NjRrI7svgCbrj6e1H788UezCu/FixeT7Gf113YCAJIOAQWAZEkvfPWiVbc0adJI8eLFZeTIkXL79u1Ef+9FixbJqFGj/PLiunDhwqw0DQBIUKkT9uUAwH88+eSTMmPGDImMjJTvvvtOevbsKUFBQRIWFhbr3Js3b5rAIyFkz549QV4HAAA7IEMBINkKDg6WvHnzSqFChaRHjx5St25d+fbbbz2G7rz55pty3333SalSpcz+v/76S1q1aiVZs2Y1gUHTpk3l6NGjrteMioqSvn37muM5cuSQV199VWKuDxpzyJMGNAMHDpQCBQqYNmm2ZPr06eZ1a9WqZc7Jli2byVRou1R0dLSMHj1aihQpIunSpZOKFSvKl19+6fE+GiSVLFnSHNfXcW+nL/Rn69Kli+s99XcyceLEOM8dMWKE5MqVSzJnziwvvPCCCcic4tN2AEDyQYYCQIqhF7fnz593PV69erW5IF65cqV5fOvWLalfv75Ur15dfvrpJ0mdOrW88cYbJtOxd+9ek8EYN26czJw5Uz799FMpU6aMefz1119L7dq17/i+7du3l82bN8ukSZPMxXV4eLicO3fOBBhfffWVtGjRQg4dOmTaom1UekE+e/ZsmTp1qpQoUULWr18vbdu2NRfxNWvWNIFP8+bNTdalW7dusmPHDunXr9+/+v1oIJA/f35ZuHChCZY2bdpkXjtfvnwmyHL/vaVNm9YM19IgplOnTuZ8Dc7i03YAQDLjAIBkqEOHDo6mTZua+9HR0Y6VK1c6goODHf3793cdz5MnjyMyMtL1nM8//9xRqlQpc76THk+XLp3jhx9+MI/z5cvnGDNmjOv4rVu3HPnz53e9l6pZs6bj5ZdfNvcPHTqk6Qvz/nFZu3atOR4REeHad+PGDUf69OkdmzZt8ji3S5cujtatW5v7YWFhjrJly3ocHzhwYKzXiqlQoUKOCRMmOOKrZ8+ejhYtWrge6+8te/bsjqtXr7r2ffjhh46MGTM6oqKi4tX2uH5mAIB9kaEAkGwtXbpUMmbMaDIP+u37c889J8OHD3cdDwkJ8aib2LNnj/zxxx+SKVMmj9e5ceOGHD58WC5duiSnTp2SqlWruo5pFuPBBx+MNezJaffu3ZIqVSqvvpnXNly7dk2eeOIJj/06rKhy5crm/sGDBz3aoTSz8m+9//77Jvty7NgxuX79unnPSpUqeZyjWZb06dN7vO+VK1dM1kRv79V2AEDyQkABINnSuoIPP/zQBA1aJ6EX/+4yZMjg8VgvhqtUqSJz5syJ9Vo6XMcXziFM3tB2qGXLlsn999/vcUxrMBLLvHnzpH///mYYlwYJGliNHTtWtm7d6vdtBwBYh4ACQLKlAYMWQMfXAw88IPPnz5fcuXObeoa4aD2BXmA/9thj5rFOQ7tz507z3LhoFkSzI+vWrTNF4TE5MyRaEO1UtmxZc/GtWYI7ZTa0fsNZYO60ZcsW+Tc2btwoNWrUkBdffNG1TzMzMWkmR7MXzmBJ31czQVoTooXs92o7ACB5YZYnAPh/bdq0kZw5c5qZnbQoW4untfC4d+/ecvz4cXPOyy+/LG+//bYsXrxYfv31V3Pxfbc1JHTdhw4dOkjnzp3Nc5yvuWDBAnNcZ6DS2Z10eNbZs2fNN/yaGdBMQZ8+fWTWrFnmon7Xrl0yefJk81jpzEq///67DBgwwBR0z5071xSLx8eJEyfMUCz3LSIiwhRQa3H3Dz/8IL/99psMGTJEtm/fHuv5OnxJZ4M6cOCAmWlq2LBh0qtXLwkMDIxX2wEAyQsBBQD8P60L0BmJChYsaGZQ0iyAXjhrDYUzY6EzKbVr184ECc5hQc2aNbvr6+qwq6efftoEH6VLl5auXbvK1atXzTEdFqRTsL722muSJ08ec2GudGE8vaDXGZO0HTrTlA4j0qlYlbZRZ4jSIEVrGnRGpbfeeiteP+e7775r6hncN33t7t27m5/7mWeeMfUZOiOWe7bCqU6dOib40CyNntukSROP2pR7tR0AkLwEaGW21Y0AAAAAYE9kKAAAAAD4jIACAAAAgM8IKAAAAAD4jIACAAAAgM8IKAAAAAD4jIACAAAAgM8IKAAAAAD4jIACAAAAgM8IKAAAAAD4jIACAAAAgM8IKAAAAACIr/4PAvw7RgRleJgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "Configuration Results:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Layer 1  | Layer 2  | Layer 3  |  Epochs  |  Val Accuracy   |  Test Accuracy  |  Success  \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "   32    |    64    |    32    |   944    | 1.0000          | 1.0000          | ✓\n",
      "\n",
      "Best Configuration:\n",
      "- Layer 1: 32 neurons\n",
      "- Layer 2: 64 neurons\n",
      "- Layer 3: 32 neurons\n",
      "- Trained for 944 epochs\n",
      "- Validation Accuracy: 1.0000\n",
      "- Test Accuracy: 1.0000\n",
      "- Success: Yes\n",
      "\n",
      "Activation function analysis:\n",
      "\n",
      "Activation Function Analysis:\n",
      "----------------------------------------\n",
      "      Tanh:   34.13%\n",
      " LeakyReLU:   32.80%\n",
      "    Linear:   33.06%\n",
      "\n",
      "Total weight magnitude: 1.243132\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class ArithmeticClassifier(nn.Module):\n",
    "   def __init__(self, first_layer_size, second_layer_size, third_layer_size):\n",
    "       super(ArithmeticClassifier, self).__init__()\n",
    "       \n",
    "       # Input layer (3 inputs: a, b, c where c = operation(a, b))\n",
    "       self.input = nn.Linear(3, first_layer_size)\n",
    "       \n",
    "       # Triple activation for first hidden layer\n",
    "       self.tanh = nn.Tanh()\n",
    "       self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "       self.identity = nn.Identity()\n",
    "       \n",
    "       # Weights for activation functions\n",
    "       self.act_weights = nn.Parameter(torch.ones(3, first_layer_size) / 3)\n",
    "       \n",
    "       # Second hidden layer\n",
    "       self.hidden = nn.Linear(first_layer_size, second_layer_size)\n",
    "       self.hidden_activation = nn.ReLU()\n",
    "       \n",
    "       # Third hidden layer\n",
    "       self.hidden2 = nn.Linear(second_layer_size, third_layer_size)\n",
    "       self.hidden_activation2 = nn.ReLU()\n",
    "       \n",
    "       # Output layer (4 classes: add, subtract, multiply, divide)\n",
    "       self.output = nn.Linear(third_layer_size, 4)\n",
    "       \n",
    "   def forward(self, x):\n",
    "       # First hidden layer with triple activation\n",
    "       x = self.input(x)\n",
    "       \n",
    "       # Apply weighted activations\n",
    "       activated = (self.tanh(x) * self.act_weights[0].unsqueeze(0) + \n",
    "                   self.leaky_relu(x) * self.act_weights[1].unsqueeze(0) +\n",
    "                   self.identity(x) * self.act_weights[2].unsqueeze(0))\n",
    "       \n",
    "       # Second hidden layer\n",
    "       x = self.hidden(activated)\n",
    "       x = self.hidden_activation(x)\n",
    "       \n",
    "       # Third hidden layer\n",
    "       x = self.hidden2(x)\n",
    "       x = self.hidden_activation2(x)\n",
    "       \n",
    "       # Output layer\n",
    "       return self.output(x)\n",
    "\n",
    "def analyze_activations(model):\n",
    "   # Get weights assigned to each activation function\n",
    "   with torch.no_grad():\n",
    "       weights = model.act_weights.detach().cpu()\n",
    "       \n",
    "       # Calculate importance of each activation\n",
    "       activation_names = ['Tanh', 'LeakyReLU', 'Linear']\n",
    "       importance = torch.mean(torch.abs(weights), dim=1)\n",
    "       \n",
    "       # Normalize to percentage\n",
    "       total = torch.sum(importance)\n",
    "       if total > 0:\n",
    "           importance = 100 * importance / total\n",
    "           \n",
    "           print(\"\\nActivation Function Analysis:\")\n",
    "           print(\"-\" * 40)\n",
    "           for i, name in enumerate(activation_names):\n",
    "               print(f\"{name:>10}: {importance[i].item():>7.2f}%\")\n",
    "               \n",
    "           print(f\"\\nTotal weight magnitude: {total.item():.6f}\")\n",
    "       else:\n",
    "           print(\"\\nActivation Function Analysis:\")\n",
    "           print(\"-\" * 40)\n",
    "           print(\"All activation weights are zero\")\n",
    "\n",
    "def generate_data(num_samples=1000):\n",
    "   \"\"\"Generate training data for arithmetic operations\"\"\"\n",
    "   X = []\n",
    "   y = []\n",
    "   operations = ['add', 'subtract', 'multiply', 'divide']\n",
    "   \n",
    "   while len(X) < num_samples:\n",
    "       # Use integers for a and b\n",
    "       a = np.random.randint(-10, 11)\n",
    "       b = np.random.randint(-10, 11)\n",
    "       \n",
    "       # Avoid division by zero\n",
    "       if b == 0:\n",
    "           b = 1\n",
    "       \n",
    "       # For division operation, ensure it produces integer results\n",
    "       if a % b != 0:\n",
    "           continue\n",
    "       \n",
    "       # Calculate results for all operations\n",
    "       results = {\n",
    "           'add': a + b,\n",
    "           'subtract': a - b,\n",
    "           'multiply': a * b,\n",
    "           'divide': a / b\n",
    "       }\n",
    "       \n",
    "       # Check if any two operations produce the same result\n",
    "       values = list(results.values())\n",
    "       if len(set(values)) == len(values):\n",
    "           # No ambiguity, randomly select an operation\n",
    "           selected_op = np.random.choice(operations)\n",
    "           c = results[selected_op]\n",
    "           \n",
    "           # Add to dataset\n",
    "           X.append([a, b, c])\n",
    "           y.append(operations.index(selected_op))\n",
    "   \n",
    "   return np.array(X), np.array(y)\n",
    "\n",
    "def train_and_evaluate(first_layer_size, second_layer_size, third_layer_size, max_epochs=2000):\n",
    "   torch.manual_seed(42)\n",
    "   np.random.seed(42)\n",
    "   \n",
    "   # Create model\n",
    "   model = ArithmeticClassifier(first_layer_size, second_layer_size, third_layer_size)\n",
    "   optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-6)\n",
    "   criterion = nn.CrossEntropyLoss()\n",
    "   \n",
    "   # Generate data\n",
    "   X_train, y_train = generate_data(5000)\n",
    "   X_val, y_val = generate_data(1000)\n",
    "   X_test, y_test = generate_data(1000)\n",
    "   \n",
    "   # Convert to tensors\n",
    "   X_train_tensor = torch.FloatTensor(X_train)\n",
    "   y_train_tensor = torch.LongTensor(y_train)\n",
    "   X_val_tensor = torch.FloatTensor(X_val)\n",
    "   y_val_tensor = torch.LongTensor(y_val)\n",
    "   X_test_tensor = torch.FloatTensor(X_test)\n",
    "   y_test_tensor = torch.LongTensor(y_test)\n",
    "   \n",
    "   best_val_accuracy = 0\n",
    "   best_model_state = None\n",
    "   patience = 100\n",
    "   no_improve = 0\n",
    "   \n",
    "   train_losses = []\n",
    "   val_accuracies = []\n",
    "   \n",
    "   # Training loop\n",
    "   for epoch in range(max_epochs):\n",
    "       # Train\n",
    "       model.train()\n",
    "       optimizer.zero_grad()\n",
    "       outputs = model(X_train_tensor)\n",
    "       loss = criterion(outputs, y_train_tensor)\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "       \n",
    "       train_losses.append(loss.item())\n",
    "       \n",
    "       # Evaluate\n",
    "       model.eval()\n",
    "       with torch.no_grad():\n",
    "           val_outputs = model(X_val_tensor)\n",
    "           _, predicted = torch.max(val_outputs, 1)\n",
    "           val_accuracy = (predicted == y_val_tensor).sum().item() / y_val_tensor.size(0)\n",
    "           val_accuracies.append(val_accuracy)\n",
    "           \n",
    "           # Print progress occasionally\n",
    "           if epoch % 50 == 0:\n",
    "               print(f\"  Epoch {epoch}: Train Loss: {loss.item():.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "           \n",
    "           # Track best model\n",
    "           if val_accuracy > best_val_accuracy:\n",
    "               best_val_accuracy = val_accuracy\n",
    "               best_model_state = model.state_dict().copy()\n",
    "               no_improve = 0\n",
    "           else:\n",
    "               no_improve += 1\n",
    "           \n",
    "           # Check if we've reached target accuracy\n",
    "           if val_accuracy >= 0.999:\n",
    "               print(f\"✓ Target achieved with layers ({first_layer_size}, {second_layer_size}, {third_layer_size}) at epoch {epoch+1}\")\n",
    "               print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "               \n",
    "               # Load best model\n",
    "               if best_model_state:\n",
    "                   model.load_state_dict(best_model_state)\n",
    "               \n",
    "               # Final test evaluation\n",
    "               test_outputs = model(X_test_tensor)\n",
    "               _, predicted = torch.max(test_outputs, 1)\n",
    "               test_accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "               \n",
    "               return {\n",
    "                   'model': model,\n",
    "                   'layer1': first_layer_size,\n",
    "                   'layer2': second_layer_size,\n",
    "                   'layer3': third_layer_size,\n",
    "                   'epochs': epoch+1,\n",
    "                   'val_accuracy': val_accuracy,\n",
    "                   'test_accuracy': test_accuracy,\n",
    "                   'train_losses': train_losses,\n",
    "                   'val_accuracies': val_accuracies,\n",
    "                   'success': True,\n",
    "                   'X_test': X_test,\n",
    "                   'y_test': y_test,\n",
    "                   'y_pred': predicted.cpu().numpy()\n",
    "               }\n",
    "           \n",
    "           # Early stopping\n",
    "           if no_improve >= patience:\n",
    "               print(f\"× Early stopping at epoch {epoch+1}\")\n",
    "               print(f\"  Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "               \n",
    "               # Load best model\n",
    "               if best_model_state:\n",
    "                   model.load_state_dict(best_model_state)\n",
    "               \n",
    "               # Final test evaluation\n",
    "               test_outputs = model(X_test_tensor)\n",
    "               _, predicted = torch.max(test_outputs, 1)\n",
    "               test_accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "               \n",
    "               return {\n",
    "                   'model': model,\n",
    "                   'layer1': first_layer_size,\n",
    "                   'layer2': second_layer_size,\n",
    "                   'layer3': third_layer_size,\n",
    "                   'epochs': epoch+1,\n",
    "                   'val_accuracy': best_val_accuracy,\n",
    "                   'test_accuracy': test_accuracy,\n",
    "                   'train_losses': train_losses,\n",
    "                   'val_accuracies': val_accuracies,\n",
    "                   'success': best_val_accuracy >= 0.999,\n",
    "                   'X_test': X_test,\n",
    "                   'y_test': y_test,\n",
    "                   'y_pred': predicted.cpu().numpy()\n",
    "               }\n",
    "   \n",
    "   # Load best model before final evaluation\n",
    "   if best_model_state:\n",
    "       model.load_state_dict(best_model_state)\n",
    "   \n",
    "   # Final test evaluation\n",
    "   model.eval()\n",
    "   with torch.no_grad():\n",
    "       test_outputs = model(X_test_tensor)\n",
    "       _, predicted = torch.max(test_outputs, 1)\n",
    "       test_accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "   \n",
    "   print(f\"× Reached max epochs ({max_epochs})\")\n",
    "   print(f\"  Best validation accuracy: {best_val_accuracy:.4f}, Test accuracy: {test_accuracy:.4f}\")\n",
    "   \n",
    "   return {\n",
    "       'model': model,\n",
    "       'layer1': first_layer_size,\n",
    "       'layer2': second_layer_size,\n",
    "       'layer3': third_layer_size,\n",
    "       'epochs': max_epochs,\n",
    "       'val_accuracy': best_val_accuracy,\n",
    "       'test_accuracy': test_accuracy,\n",
    "       'train_losses': train_losses,\n",
    "       'val_accuracies': val_accuracies,\n",
    "       'success': best_val_accuracy >= 0.999,\n",
    "       'X_test': X_test,\n",
    "       'y_test': y_test,\n",
    "       'y_pred': predicted.cpu().numpy()\n",
    "   }\n",
    "\n",
    "def plot_confusion_matrix(result):\n",
    "   \"\"\"Plot confusion matrix for the test results\"\"\"\n",
    "   operations = ['Addition', 'Subtraction', 'Multiplication', 'Division']\n",
    "   \n",
    "   # Calculate confusion matrix\n",
    "   cm = confusion_matrix(result['y_test'], result['y_pred'])\n",
    "   \n",
    "   # Plot\n",
    "   plt.figure(figsize=(10, 8))\n",
    "   sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=operations, yticklabels=operations)\n",
    "   plt.ylabel('True Label')\n",
    "   plt.xlabel('Predicted Label')\n",
    "   plt.title('Confusion Matrix')\n",
    "   plt.show()\n",
    "\n",
    "# Test the best configuration from previous run\n",
    "print(\"Training the best configuration for visualization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Best configuration\n",
    "layer1 = 32\n",
    "layer2 = 64\n",
    "layer3 = 32\n",
    "\n",
    "result = train_and_evaluate(layer1, layer2, layer3)\n",
    "analyze_activations(result['model'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(result)\n",
    "\n",
    "# Show summary\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"Configuration Results:\")\n",
    "print(\"-\" * 120)\n",
    "print(f\"{'Layer 1':^8} | {'Layer 2':^8} | {'Layer 3':^8} | {'Epochs':^8} | {'Val Accuracy':^15} | {'Test Accuracy':^15} | {'Success':^10}\")\n",
    "print(\"-\" * 120)\n",
    "print(f\"{result['layer1']:^8} | {result['layer2']:^8} | {result['layer3']:^8} | {result['epochs']:^8} | {result['val_accuracy']:.4f}{' ':^9} | {result['test_accuracy']:.4f}{' ':^9} | {'✓' if result['success'] else '×'}\")\n",
    "\n",
    "print(\"\\nBest Configuration:\")\n",
    "print(f\"- Layer 1: {result['layer1']} neurons\")\n",
    "print(f\"- Layer 2: {result['layer2']} neurons\")\n",
    "print(f\"- Layer 3: {result['layer3']} neurons\")\n",
    "print(f\"- Trained for {result['epochs']} epochs\")\n",
    "print(f\"- Validation Accuracy: {result['val_accuracy']:.4f}\")\n",
    "print(f\"- Test Accuracy: {result['test_accuracy']:.4f}\")\n",
    "print(f\"- Success: {'Yes' if result['success'] else 'No'}\")\n",
    "\n",
    "print(\"\\nActivation function analysis:\")\n",
    "analyze_activations(result['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2d229e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_operation(model, numbers):\n",
    "    \"\"\"Predict the arithmetic operation given a tuple of (a, b, c)\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ArithmeticClassifier model\n",
    "        numbers: Tuple or list containing (a, b, c) where c is the result of an operation on a and b\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (operation, result_string, correctness)\n",
    "    \"\"\"\n",
    "    operations = ['add', 'subtract', 'multiply', 'divide']\n",
    "    operation_symbols = ['+', '-', '*', '/']\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.FloatTensor([numbers])\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        operation_idx = predicted.item()\n",
    "    \n",
    "    # Get predicted operation\n",
    "    operation = operations[operation_idx]\n",
    "    symbol = operation_symbols[operation_idx]\n",
    "    \n",
    "    # Verify if prediction makes sense\n",
    "    a, b, c = numbers\n",
    "    expected_results = {\n",
    "        'add': a + b,\n",
    "        'subtract': a - b,\n",
    "        'multiply': a * b,\n",
    "        'divide': a / b if b != 0 else float('inf')\n",
    "    }\n",
    "    \n",
    "    # Format the result\n",
    "    result = f\"{a} {symbol} {b} = {c}\"\n",
    "    confidence = \"Correct\" if abs(expected_results[operation] - c) < 1e-6 else \"Incorrect\"\n",
    "    \n",
    "    return operation, result, confidence\n",
    "\n",
    "# Example usage\n",
    "def test_prediction(model):\n",
    "    \"\"\"Test the model with some example inputs\"\"\"\n",
    "    test_cases = [\n",
    "        (5, 3, 8),     # Addition: 5 + 3 = 8\n",
    "        (10, 4, 6),    # Subtraction: 10 - 4 = 6\n",
    "        (3, 5, 15),    # Multiplication: 3 * 5 = 15\n",
    "        (8, 2, 4)      # Division: 8 / 2 = 4\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting prediction with examples:\\n\" + \"-\" * 40)\n",
    "    for test in test_cases:\n",
    "        operation, result, confidence = predict_operation(model, test)\n",
    "        print(f\"Input: {test}\")\n",
    "        print(f\"Predicted: {operation} ({result})\")\n",
    "        print(f\"Validation: {confidence}\\n\")\n",
    "    \n",
    "    # Allow user to input custom examples\n",
    "    print(\"Enter your own examples (type 'exit' to quit):\")\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"Enter three numbers (a,b,c) separated by commas: \")\n",
    "            if user_input.lower() == 'exit':\n",
    "                break\n",
    "                \n",
    "            a, b, c = map(float, user_input.split(','))\n",
    "            operation, result, confidence = predict_operation(model, (a, b, c))\n",
    "            print(f\"Predicted: {operation} ({result})\")\n",
    "            print(f\"Validation: {confidence}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Please try again with the format: number,number,number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "802612d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: divide (100 / 2.5 = 40)\n",
      "Validation: Correct\n"
     ]
    }
   ],
   "source": [
    "operation, z, confidence = predict_operation(result['model'], (100, 2.5, 40))\n",
    "print(f\"Predicted: {operation} ({z})\")\n",
    "print(f\"Validation: {confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d428afa5",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "1. Perfect 100%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
